{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uPulTbeerl8"
      },
      "source": [
        "# Init code to get the data needed for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HB_BhhBzeqXj",
        "outputId": "8cdf5457-ec6b-4c93-d06f-29053284a53f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-14 21:01:38--  https://r2.37infos.com/en-sv-train.txt\n",
            "Resolving r2.37infos.com (r2.37infos.com)... 104.21.71.47, 172.67.169.121, 2606:4700:3032::ac43:a979, ...\n",
            "Connecting to r2.37infos.com (r2.37infos.com)|104.21.71.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1157960 (1.1M) [text/plain]\n",
            "Saving to: ‘en-sv-train.txt’\n",
            "\n",
            "en-sv-train.txt     100%[===================>]   1.10M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-05-14 21:01:38 (25.4 MB/s) - ‘en-sv-train.txt’ saved [1157960/1157960]\n",
            "\n",
            "--2024-05-14 21:01:38--  https://r2.37infos.com/en-sv-valid.txt\n",
            "Resolving r2.37infos.com (r2.37infos.com)... 104.21.71.47, 172.67.169.121, 2606:4700:3032::ac43:a979, ...\n",
            "Connecting to r2.37infos.com (r2.37infos.com)|104.21.71.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128898 (126K) [text/plain]\n",
            "Saving to: ‘en-sv-valid.txt’\n",
            "\n",
            "en-sv-valid.txt     100%[===================>] 125.88K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-14 21:01:38 (5.63 MB/s) - ‘en-sv-valid.txt’ saved [128898/128898]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://r2.37infos.com/en-sv-train.txt\n",
        "!wget https://r2.37infos.com/en-sv-valid.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8SC3p-UCusw"
      },
      "source": [
        "# RNN Machine Translation Laboration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvKNkEYrCusy"
      },
      "source": [
        "In this lab, your task is to build a sequence-to-sequence model, using recurrent neural networks, that translates short sentences from Swedish into English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "FwTmu8E2TICb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Tensorflow is quite chatty; filter out warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "IDNS-GhLCusz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCU1l0GeTICc"
      },
      "source": [
        "### Check that Tensorflow uses a GPU _(optional)_\n",
        "\n",
        "Training the models in this notebook can be sped up significantly with a GPU.  The following cell can be used to check if the GPU is set up correctly.  If you run on CPU, you can either run or just ignore this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zE6a9TiLTICc",
        "outputId": "7237309a-c4ff-4843-bf2d-4aa154302ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Tensorflow has detected a GPU.\n",
            "\n",
            "✗ XLA_FLAGS not set. If you encounter errors during training, you might have to set\n",
            "        XLA_FLAGS=\"--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/\"\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "# The GPU id to use, usually either \"0\" or \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "if tf.config.list_physical_devices(\"GPU\"):\n",
        "    print(\"✓ Tensorflow has detected a GPU.\")\n",
        "    import shutil\n",
        "    if not shutil.which(\"ptxas\"):\n",
        "        print(\"\\n✗ Command 'ptxas' not found in path -- you might have to install `cudatoolkit-dev`\")\n",
        "    if not \"XLA_FLAGS\" in os.environ:\n",
        "        print(\"\\n✗ XLA_FLAGS not set. If you encounter errors during training, you might have to set\")\n",
        "        print(\"        XLA_FLAGS=\\\"--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/\\\"\")\n",
        "\n",
        "    # Allow growth of GPU memory, otherwise it will always look like all the memory is being used\n",
        "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "else:\n",
        "    print(\"✗ Tensorflow has NOT detected a GPU.\")\n",
        "    print()\n",
        "    print(\"For GPU support, visit: <https://www.tensorflow.org/install/pip>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGF_l9MNTICd"
      },
      "source": [
        "## Problem Specification\n",
        "\n",
        "Your task in this assignment is to:\n",
        "\n",
        "1. Build an encoder—decoder model based on recurrent neural networks.\n",
        "2. Train this model on the provided training data, a collection of parallel Swedish–English sentences.\n",
        "3. Evaluate the performance of this model on the provided test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX-5kU7mCus1"
      },
      "source": [
        "### The data: Swedish–English Anki corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luSjyoOqCus2"
      },
      "source": [
        "The data in this lab consists of bilingual Swedish–English sentence pairs from the [Tatoeba Project](https://tatoeba.org/en) as collected by [Anki](http://www.manythings.org/anki/).  These are comparatively short sentences, suitable for language learners, and therefore also well-suited for building a small machine translation model. Here are some example sentences from the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EaFUyzF-Cus2",
        "outputId": "8093e399-e020-4971-8465-c112c8488479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i returned to japan .', 'jag återvände till japan .']\n",
            "['i love her .', 'jag älskar henne .']\n",
            "[\"this is tom ' s school .\", 'detta är toms skola .']\n",
            "['i can hardly stand .', 'jag kan knappt stå .']\n"
          ]
        }
      ],
      "source": [
        "with open(\"en-sv-train.txt\", \"rt\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        pair = [sent for sent in line.rstrip().split(\"\\t\")]\n",
        "        print(pair)\n",
        "        if i > 2:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVYZFLVyCus3"
      },
      "source": [
        "Each line in the data files consists of an English–Swedish sentence pair. The sentences are already lower-cased and pre-tokenized (using the [toktok tokenizer from NLTK](https://www.nltk.org/howto/tokenize.html)), so we can simply split them up by whitespace to get sequences of tokens.  To make your life a bit easier, we have removed sentences longer than 15 words.\n",
        "\n",
        "The next cell contains code that yields the sentences contained in a file as lists of strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Eay_PCwACus4",
        "outputId": "c6b72696-ddd6-4eee-c4c9-f769dfe26559"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'returned', 'to', 'japan', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "ENGLISH = 0\n",
        "SWEDISH = 1\n",
        "\n",
        "def sentences(filename, idx):\n",
        "    # Use idx=0 for English, idx=1 for Swedish\n",
        "    with open(filename, \"rt\") as source:\n",
        "        for line in source:\n",
        "            yield line.rstrip().split(\"\\t\")[idx].split()\n",
        "\n",
        "# Example usage\n",
        "next(sentences(\"en-sv-train.txt\", ENGLISH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypi63gGlCus5"
      },
      "source": [
        "## Part 1: Build the vocabularies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REbT89KqCus5"
      },
      "source": [
        "Before we can feed them into any model, we first need to convert the text strings to integers. For this purpose, we'll create a **vocabulary** of tokens that are known to the model, one vocabulary for each language. We need four **special tokens (or \"pseudowords\")**:\n",
        "\n",
        "1. `<pad>` at index 0 for padding purposes\n",
        "2. `<unk>` at index 1 to represent unknown words\n",
        "3. `<bos>` at index 2 to mark the \"beginning of sequence\" in the decoder\n",
        "4. `<eos>` at index 3 to mark the \"end of sequence\" in the decoder\n",
        "\n",
        "The remaining items in the vocabulary should be made up of the **most frequent words** in the training data for the respective language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqqYWzlDTICg"
      },
      "source": [
        "#### 🤔 Task 1: Write the function to build the vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "R29hmyaRCus5"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def make_vocab(sentences, max_size):\n",
        "    \"\"\"Return a list of the `max_size` most frequent tokens in `sentences`.\"\"\"\n",
        "    \"\"\"\n",
        "    # 1. Count of how often each word occurs in the data\n",
        "    # 2. Sort the words by their frequency, in descending order\n",
        "    # 3. Make a list of the special tokens plus the most frequent words, up to a length of `max_size`.\n",
        "    # 4. Return the list\n",
        "\n",
        "    \"\"\"\n",
        "    word_bag = {}\n",
        "    for sentense in sentences:\n",
        "      for word in sentense:\n",
        "        if word in word_bag:\n",
        "          word_bag[word] += 1\n",
        "        else:\n",
        "          word_bag[word] = 1\n",
        "\n",
        "    sorted_word_bag = dict(sorted(word_bag.items(), key=lambda item: item[1], reverse=True))\n",
        "    word_bag_list = list(sorted_word_bag.keys())\n",
        "\n",
        "    result = [\"<pad>\",\"<unk>\",\"<bos>\",\"<eos>\"]\n",
        "    result.extend(word_bag_list)\n",
        "\n",
        "    return(result[:max_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmr7Oe9cCus7"
      },
      "source": [
        "With this function, we can construct vocabularies containing the 5,000 most frequent words as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "onyml-oDCus7"
      },
      "outputs": [],
      "source": [
        "src_vocab = make_vocab(sentences('en-sv-train.txt', SWEDISH), 5000)\n",
        "tgt_vocab = make_vocab(sentences('en-sv-train.txt', ENGLISH), 5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8UsAWXlZBOJ7",
        "outputId": "187456c2-728b-4cdf-961b-dd30ff7472d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', '<unk>', '<bos>', '<eos>', '.', 'jag', 'är', '?', 'tom', 'det']\n",
            "['<pad>', '<unk>', '<bos>', '<eos>', '.', 'i', \"'\", 'tom', '?', 'you']\n"
          ]
        }
      ],
      "source": [
        "print(src_vocab[:10])\n",
        "print(tgt_vocab[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX9T47dnCus8"
      },
      "source": [
        "#### 🤞 Test your code\n",
        "\n",
        "To test your code, check that each vocabulary contains 5,000 words, and includes the pseudowords at the right positions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tZBil9ezCus8",
        "outputId": "e8df2c5a-43a4-4c97-bd4c-15969b3795a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All good!\n"
          ]
        }
      ],
      "source": [
        "def test1():\n",
        "    assert len(src_vocab) == 5000\n",
        "    assert len(tgt_vocab) == 5000\n",
        "    assert src_vocab[:4] == ['<pad>', '<unk>', '<bos>', '<eos>']\n",
        "    assert tgt_vocab[:4] == ['<pad>', '<unk>', '<bos>', '<eos>']\n",
        "    print(\"All good!\")\n",
        "\n",
        "test1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzqu2vJWTICh"
      },
      "source": [
        "### Wrap the vocabularies in StringLookup layers\n",
        "\n",
        "For mapping tokens to their vocabulary IDs, we can use Keras' `StringLookup` layer. The next cell constructs layers for both the source and target vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "nuGnXniiTICh"
      },
      "outputs": [],
      "source": [
        "string_lookup_args = dict(output_mode=\"int\", mask_token=\"<pad>\", oov_token=\"<unk>\")\n",
        "src_lookup = layers.StringLookup(vocabulary=src_vocab, **string_lookup_args)\n",
        "tgt_lookup = layers.StringLookup(vocabulary=tgt_vocab, **string_lookup_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW0lrSxPTICh"
      },
      "source": [
        "The next cell gives an example how these `StringLookup` layers can be used. Note that the layers already return *tensors*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_k02qDywTICh",
        "outputId": "897dcfdd-a79b-4d59-95f9-29a2bb726fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([   5 1005   11  530  188    4], shape=(6,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "example = \"i returned to japan yesterday .\".split()\n",
        "print(tgt_lookup(example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egRjxcuqTICh"
      },
      "source": [
        "#### 🤔 Task 2: Sanity-check that these numbers are correct\n",
        "\n",
        "Check your understanding of what's happening in the `StringLookup` layer by writing two lines of code:\n",
        "1. One that prints the token corresponding to the _second integer_ in the tensor above.\n",
        "2. One that prints the integer corresponding to the _second word_ (\"returned\") in the example above.\n",
        "\n",
        "Use `tgt_vocab` directly for that, not the lookup layer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "emWccRD6TICh",
        "outputId": "91acfc08-8432-46a5-ce74-e667c9120b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "returned\n",
            "1005\n"
          ]
        }
      ],
      "source": [
        "## TODO: Your code here\n",
        "# tgt_lookup(example)[1].numpy().item() will generate 2nd integer 1005\n",
        "# and the corresponding word is [returned]\n",
        "second_int = tgt_lookup(example)[1].numpy().item()\n",
        "second_word = example[1]\n",
        "\n",
        "print(tgt_vocab[second_int])\n",
        "print([index for index, string in enumerate(tgt_vocab) if second_word == string][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2fQzgkPCus9"
      },
      "source": [
        "### Wrapping everything in data loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MF4JRJVCus9"
      },
      "source": [
        "The next cell defines a function that wraps our dataset in TensorFlow's [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) API, which represents map-style datasets. The advantage of this is that it lets us use standard infrastructure related to the loading and automatic batching of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "TkFTb0LkCus9"
      },
      "outputs": [],
      "source": [
        "def append_eos(tensor):\n",
        "    \"Helper function that appends '<eos>' to a sequence.\"\n",
        "    return tf.concat([tensor, tf.constant([\"<eos>\"], dtype=tf.string)], axis=0)\n",
        "\n",
        "def load_translation_dataset(src_lookup, tgt_lookup, filename):\n",
        "    # Build source dataset and convert with src_lookup\n",
        "    src_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        tf.ragged.constant(list(sentences(filename, SWEDISH)))\n",
        "    )\n",
        "    src_dataset = src_dataset.map(src_lookup)\n",
        "\n",
        "    # Build target dataset, append <eos> and convert with tgt_lookup\n",
        "    tgt_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        tf.ragged.constant(list(sentences(filename, ENGLISH)))\n",
        "    )\n",
        "    tgt_dataset = tgt_dataset.map(append_eos).map(tgt_lookup)\n",
        "\n",
        "    # Zip them together\n",
        "    return tf.data.Dataset.zip((src_dataset, tgt_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnPlzxWrCus-"
      },
      "source": [
        "We load the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "t1tyWbfbCus-"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_translation_dataset(src_lookup, tgt_lookup, \"en-sv-train.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(train_dataset)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mSyPzBp5xgJp",
        "outputId": "b3e15baf-4030-4a25-861a-4c0f7d26310a"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(5,), dtype=int64, numpy=array([   5, 1768,   29,  505,    4])>,\n",
              " <tf.Tensor: shape=(6,), dtype=int64, numpy=array([   5, 1005,   11,  530,    4,    3])>)"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"en-sv-train.txt\"\n",
        "print(list(sentences(filename, SWEDISH))[0])\n",
        "print(list(sentences(filename, ENGLISH))[0])\n",
        "print(tgt_vocab[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FM7yGJ21yKm6",
        "outputId": "b736063d-2293-4c58-a6f7-0f1dc046da16"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jag', 'återvände', 'till', 'japan', '.']\n",
            "['i', 'returned', 'to', 'japan', '.']\n",
            "<eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoUAW-GcCus_"
      },
      "source": [
        "The following function can be helpful for debugging. It extracts a single source–target pair of sentences from the specified *dataset* and converts it into batches of size&nbsp;1, which can be fed into the encoder–decoder model. This also illustrates how the `Dataset` API works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "DC2j_oZyCus_"
      },
      "outputs": [],
      "source": [
        "def example(dataset, i):\n",
        "    if i > 0:\n",
        "        dataset = dataset.skip(i-1)\n",
        "    return list(dataset.take(1).batch(1))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dYhTBym7Cus_",
        "outputId": "8f80a3b0-74f1-46cf-8856-81b9b782a141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[  6  10 245  15  20   7]], shape=(1, 6), dtype=int64)\n",
            "tf.Tensor([[ 29   9 477  55  28   8   3]], shape=(1, 7), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "x, y = example(train_dataset, 42)\n",
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw62gfhdCutA"
      },
      "source": [
        "## Part 2: The encoder–decoder architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB280fFjCutA"
      },
      "source": [
        "In this section, you will implement the encoder–decoder architecture, including the extension of that architecture by an attention mechanism. The implementation consists of four parts: the encoder, the attention mechanism, the decoder, and a class that wraps the complete architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUFFRKwGCutA"
      },
      "source": [
        "### Part 2.1: Implement the encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKeHOZyxCutA"
      },
      "source": [
        "The encoder is a component that takes an input tensor of vocabulary IDs, like the `x` tensor from the example above, and performs the following steps:\n",
        "\n",
        "1. Look up **word embeddings** for each token in the sequence.\n",
        "2. Process them with a **bi-directional recurrent neural network**. This works with any type of RNN, but we will use **GRU (gated recurrent unit) layers** throughout this laboration.\n",
        "3. Feed the output through a linear layer. We also take the last hidden state of the forward GRU and the last hidden state of the backward GRU, concatenate them, and pass them through a linear layer. This produces a \"summary\" of the source sentence, which we will later feed into the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBzBO9KKTICj"
      },
      "source": [
        "Let's implement the encoder by defining it as a **custom Keras layer.** For this, we have to define a class that subclasses from `keras.layers.Layer`, instantiate all required model weights and/or (sub)layers in the `__init__()` function, and uses them to perform the layer's computation in the `call()` function. Below is some skeleton code to get you started; you can also [read more about making custom layers in the Keras Docs](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2uqAhxkTICn"
      },
      "source": [
        "#### 🤔 Task 3: Implement the encoder by completing the skeleton code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "klNT7vTPCutB"
      },
      "outputs": [],
      "source": [
        "class Encoder(layers.Layer):\n",
        "    def __init__(self, num_words, embedding_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        # defines the required layers/weights\n",
        "        self.embedding = layers.Embedding(input_dim=num_words, output_dim=embedding_dim)\n",
        "        self.rnn = layers.Bidirectional(layers.GRU(hidden_dim, return_sequences=True, return_state=True))\n",
        "        self.linear = layers.Dense(hidden_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        # Look up the embeddings for the source words\n",
        "        embedded = self.embedding(inputs)\n",
        "\n",
        "        # Apply a bi-directional GRU over the source sequences\n",
        "        rnn_output, forward_state, backward_state = self.rnn(embedded)\n",
        "\n",
        "        # Concatenate forward and backward hidden states\n",
        "        concat_states = tf.concat([forward_state, backward_state], axis=1)  # Concatenate along the second axis\n",
        "\n",
        "        # Apply a linear transformation to the concatenated states\n",
        "        linear_output = self.linear(concat_states)\n",
        "        return rnn_output, linear_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut1QP_1hCutB"
      },
      "source": [
        "Your code must comply with the following specification:\n",
        "\n",
        "**__init__** (*num_words*, *embedding_dim* = 128, *hidden_dim* = 256)\n",
        "\n",
        "> Initialises the encoder. The encoder consists of an embedding layer that maps each of *num_words* words to an embedding vector of size *embedding_dim*, a bidirectional GRU that maps each embedding vector to a position-specific representation of size 2 × *hidden_dim*, and a final linear layer that projects these representations to new representations of size *hidden_dim*.\n",
        "\n",
        "**call** (*self*, *inputs*)\n",
        "\n",
        "> Takes a tensor *inputs* with source-language word ids and sends it through the encoder. The input tensor has shape (*batch_size*, *src_len*), where *src_len* is the length of the sentences in the batch. (We will make sure that all sentences in the same batch have the same length.) The method returns a pair of tensors (*output*, *hidden*), where *output* has shape (*batch_size*, *src_len*, *hidden_dim*), and *hidden* has shape (*batch_size*, *hidden_dim*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xybpq39KCutC"
      },
      "source": [
        "#### 🤞 Test your code\n",
        "\n",
        "To test your code, instantiate an encoder, feed it the first source sentence in the training data, and check that the tensors returned by the encoder have the expected shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LU4SIh0NCutC",
        "outputId": "2512aa47-6f7b-4d56-b762-1f4cbf8aa2f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 6)\n",
            "(1, 6, 512)\n",
            "(1, 256)\n"
          ]
        }
      ],
      "source": [
        "def test21():\n",
        "    src, tgt = example(train_dataset, 42)\n",
        "\n",
        "    encoder = Encoder(src_lookup.vocabulary_size())\n",
        "\n",
        "    output, hidden = encoder(src)\n",
        "\n",
        "    print(src.shape)     # batch_size=1,src_len=6\n",
        "    print(output.shape)  # should be (batch_size, src_len, hidden_dim)\n",
        "    print(hidden.shape)  # should be (batch_size, hidden_dim)\n",
        "\n",
        "test21()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnD4At5uCutD"
      },
      "source": [
        "### Part 2.2: Implement the attention mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twmxl69tCutD"
      },
      "source": [
        "Your next task is to implement the attention mechanism. Recall that the purpose of this mechanism is to inform the decoder when generating the translation of the next word. For this, attention has access to the previous hidden state of the decoder, as well as the complete output of the encoder. It returns the attention-weighted sum of the encoder output, the so-called *context* vector. For later usage, we also return the attention weights.\n",
        "\n",
        "As mentioned in the lecture, attention can be implemented in various ways. One very simple implementation is *uniform attention*, which assigns equal weight to each position-specific representation in the output of the encoder, and completely ignores the hidden state of the decoder. This mechanism is implemented in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "VK4LsCUfCutD"
      },
      "outputs": [],
      "source": [
        "class UniformAttention(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, decoder_hidden, encoder_output, mask=None):\n",
        "        # Set all attention scores to the same constant value (0). After\n",
        "        # the softmax, we will have uniform weights.\n",
        "        scores = tf.zeros_like(encoder_output[:, :, -1])\n",
        "\n",
        "        # Mask out the attention scores for the padding tokens. We set\n",
        "        # them to -inf. After the softmax, we will have 0.\n",
        "        if mask is not None:\n",
        "            masked_value = -float('inf') * tf.ones_like(scores)\n",
        "            scores = tf.where(mask, scores, masked_value)\n",
        "\n",
        "        # Convert scores into weights\n",
        "        alpha = tf.nn.softmax(scores, axis=1)\n",
        "\n",
        "        # The context is the alpha-weighted sum of the encoder outputs.\n",
        "        context = tf.linalg.matmul(tf.expand_dims(alpha, axis=1), encoder_output)\n",
        "        context = tf.squeeze(context, axis=1)\n",
        "\n",
        "        return alpha, context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noYs0hSHCutE"
      },
      "source": [
        "One technical detail in this code is our use of *mask* to compute attention weights only for the ‘real’ tokens in the source sentences, but not for the padding tokens that we introduce to bring all sentences in a batch to the same length.\n",
        "\n",
        "Your task now is to implement the attention mechanism from the paper by [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473). The relevant equation is in Section&nbsp;A.1.2:\n",
        "\n",
        "$$\n",
        "a(s_{i-1}, h_j) = v^{\\top} \\tanh(W s_{i-1} + U h_j)\n",
        "$$\n",
        "\n",
        "This equation specifies how to compute the attention score (a scalar) for the previous hidden state of the decoder, denoted by $s_{i-1}$, and the $j$-th position-specific representation in the output of the encoder, denoted by $h_j$. The equation refers to three parameters: a vector $v$ and $W$ and $U$. In PyTorch, these parameters can be represented in terms of (bias-free) linear layers that are trained along with the other parameters of the model.\n",
        "\n",
        "Here is the skeleton code for this problem. As you can see, your specific task is to initialise the required parameters and to compute the attention scores (*scores*); the rest of the code is the same as for the uniform attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnS__QE_TICp"
      },
      "source": [
        "#### 🤔 Task 4: Implement Bahdanau attention by completing the skeleton code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "RyclW2osCutE"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(layers.Layer):\n",
        "    def __init__(self, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.supports_masking = True\n",
        "\n",
        "        self.w = layers.Dense(units = hidden_dim, activation='linear')\n",
        "        self.u = layers.Dense(units = hidden_dim, activation='linear')\n",
        "        self.v = layers.Dense(1, activation='linear')\n",
        "\n",
        "    def call(self, decoder_hidden, encoder_output, mask=None):\n",
        "        # Replace the next line with your own code that computes the attention scores\n",
        "        #scores = tf.zeros_like(encoder_output[:, :, -1])\n",
        "\n",
        "        fc_encoder_out = self.w(encoder_output)\n",
        "        fc_decoder_out = self.u(decoder_hidden)\n",
        "        fc_decoder_out = tf.expand_dims(fc_decoder_out, axis=1)\n",
        "        scores = self.v(tf.math.tanh(fc_encoder_out + fc_decoder_out))\n",
        "        scores = tf.squeeze(scores, axis=-1)\n",
        "\n",
        "        # ... The rest of the code is as in UniformAttention — NO NEED TO MODIFY BELOW THIS LINE!\n",
        "\n",
        "        # Mask out the attention scores for the padding tokens. We set\n",
        "        # them to -inf. After the softmax, we will have 0.\n",
        "        if mask is not None:\n",
        "            masked_value = -float('inf') * tf.ones_like(scores)\n",
        "            scores = tf.where(mask, scores, masked_value)\n",
        "\n",
        "        # Convert scores into weights\n",
        "        alpha = tf.nn.softmax(scores, axis=1)\n",
        "\n",
        "        # The context is the alpha-weighted sum of the encoder outputs.\n",
        "        context = tf.linalg.matmul(tf.expand_dims(alpha, axis=1), encoder_output)\n",
        "        context = tf.squeeze(context, axis=1)\n",
        "\n",
        "        return alpha, context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTB0RPfDCutF"
      },
      "source": [
        "Your code must comply with the following specification:\n",
        "\n",
        "**call** (*decoder_hidden*, *encoder_output*, *mask*)\n",
        "\n",
        "> Takes the previous hidden state of the decoder (*decoder_hidden*) and the encoder output (*encoder_output*) and returns a pair (*alpha*, *context*) where *context* is the context as computed as in [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473), and *alpha* are the corresponding attention weights. The hidden state has shape (*batch_size*, *hidden_dim*), the encoder output has shape (*batch_size*, *src_len*, *hidden_dim*), the context has shape (*batch_size*, *hidden_dim*), and the attention weights have shape (*batch_size*, *src_len*).\n",
        "\n",
        "#### 💡 Hints on the implementation\n",
        "\n",
        "You may need a few more \"low-level\" TensorFlow functions to implement this part, concretely:\n",
        "    \n",
        "- `tf.expand_dims()` and `tf.squeeze()` to add/remove a dimension from a tensor. This is because some tensors have a \"timestep\" dimension while others (e.g. the hidden state of the decoder) don't.\n",
        "- `tf.nn.tanh()` to compute the $\\tanh$ function on a tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxNwwlmuCutF"
      },
      "source": [
        "#### 🤞 Test your code\n",
        "\n",
        "To test your code, extend your test from Task 3: Feed the output of your encoder into your attention class. As the previous hidden state of the decoder, you can use the hidden state returned by the encoder. Later, you don't need to pass the mask explicitly (Keras will do this automatically), but for testing purposes, you can obtain the mask from a layer's output via `output._keras_mask`.\n",
        "\n",
        "Check that the context tensor and the attention weights returned by the attention class have the expected shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZUqLpHXiCutG",
        "outputId": "96255a14-a9f5-49a4-9eb9-433d80f9ab5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 6)\n",
            "(1, 512)\n"
          ]
        }
      ],
      "source": [
        "def test22():\n",
        "    src, tgt = example(train_dataset, 42)\n",
        "    encoder = Encoder(src_lookup.vocabulary_size())\n",
        "    output, hidden = encoder(src)\n",
        "    attention = BahdanauAttention()\n",
        "    alpha, context = attention(hidden, output, mask=output._keras_mask)\n",
        "    print(alpha.shape)    # should be (batch_size, src_len)\n",
        "    print(context.shape)  # should be (batch_size, hidden_dim)\n",
        "\n",
        "test22()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB2A4Qa1CutG"
      },
      "source": [
        "### Part 2.3: Implement the decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO_4A4XXCutH"
      },
      "source": [
        "Now you are ready to implement the decoder. Like the encoder, the decoder is based on a GRU; but this time we use a unidirectional network, as we generate the target sentences left-to-right.\n",
        "\n",
        "**⚠️ We expect that solving this problem will take you the longest time in this lab.**\n",
        "\n",
        "Because the decoder is an autoregressive model, we need to unroll the GRU \"manually\": At each position, we take the previous hidden state as well as the new input, and apply the GRU for one step. The initial hidden state comes from the encoder. The new input is the embedding of the previous word, concatenated with the context vector from the attention model. To produce the final output, we take the output of the GRU, concatenate the embedding vector and the context vector (residual connection), and feed the result into a linear layer. Here is a graphical representation:\n",
        "\n",
        "<img src=\"https://gitlab.liu.se/nlp/nlp-course/-/raw/master/labs/l3/decoder.svg\" width=\"50%\" alt=\"Decoder architecture\"/>\n",
        "\n",
        "We need to implement this manual unrolling for two very similar tasks: When *training*, both the inputs to and the target outputs of the GRU come from the training data. When *decoding*, the outputs of the GRU are used to generate new target-side words, and these words become the inputs to the next step of the unrolling. **We have already implemented the `call` method that handles both these two different modes of usage — you don't need to modify this.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-7sIaZpTICq"
      },
      "source": [
        "#### 🤔 Task 5: Implement the `step` method that takes a single step with the GRU\n",
        "\n",
        "You will also need to add any necessary layers/weights that you use in the `__init__` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "xBDjsJ-1CutH"
      },
      "outputs": [],
      "source": [
        "class Decoder(layers.Layer):\n",
        "    def __init__(self, trg_lookup, attention, embedding_dim=128, hidden_dim=256, max_len=16):\n",
        "        super().__init__()\n",
        "        num_words = trg_lookup.vocabulary_size()\n",
        "        self.embedding = layers.Embedding(num_words, embedding_dim, mask_zero=True)\n",
        "        self.bos_index = trg_lookup([\"<bos>\"]).numpy()\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "        self.attention = attention\n",
        "        self.rnn_cell = layers.GRU(hidden_dim, return_state=True, return_sequences=True)\n",
        "        self.linear = layers.Dense(num_words)\n",
        "\n",
        "    def call(self, encoder_output, initial_state, targets=None, training=False, mask=None):\n",
        "        # YOU WON'T NEED TO MODIFY ANYTHING IN THIS FUNCTION.\n",
        "\n",
        "        if training:\n",
        "            assert targets is not None\n",
        "\n",
        "        # Initialise the hidden state from `initial_state`\n",
        "        state = initial_state\n",
        "\n",
        "        # Initialise the decoder input with the `<bos>` symbol\n",
        "        next_input = self.bos_index * tf.ones_like(initial_state, dtype=tf.int64)\n",
        "        next_input = next_input[:, 0]\n",
        "\n",
        "        # Initialise the list of outputs and attention weights\n",
        "        outputs = tf.TensorArray(\n",
        "            tf.float32,\n",
        "            size=0 if training else self.max_len,\n",
        "            dynamic_size=training,\n",
        "        )\n",
        "        alphas = tf.TensorArray(\n",
        "            tf.float32,\n",
        "            size=0 if training else self.max_len,\n",
        "            dynamic_size=training,\n",
        "        )\n",
        "        inputs = tf.TensorArray(\n",
        "            tf.int64,\n",
        "            size=0 if training else self.max_len,\n",
        "            dynamic_size=training,\n",
        "        )\n",
        "\n",
        "        # In training mode, we iterate over the length of the target sentences,\n",
        "        # otherwise we iterate until `self.max_len` is reached\n",
        "        max_len = tf.shape(targets)[1] if training else self.max_len\n",
        "\n",
        "        for i in range(max_len):\n",
        "            # In training mode, we feed the correct (gold) predictions as the next input\n",
        "            if training and i > 0:\n",
        "                next_input = targets[:, i-1]\n",
        "\n",
        "            # Get the embedding for the previous word\n",
        "            prev_embed = self.embedding(next_input)\n",
        "\n",
        "            # Take one step with the RNN\n",
        "            step_output, state, alpha = self.step(encoder_output, state, prev_embed, mask=mask)\n",
        "\n",
        "            # Update the list of generated words and attention weights\n",
        "            outputs = outputs.write(i, step_output)\n",
        "            alphas = alphas.write(i, alpha)\n",
        "            inputs = inputs.write(i, next_input)\n",
        "\n",
        "            # Set the prediction with highest probability as the input for the next timestep\n",
        "            if not training:\n",
        "                next_input = tf.math.argmax(step_output, axis=-1)\n",
        "\n",
        "        # Lists of outputs and attention weights are [tgt_len, batch_size, *],\n",
        "        # so we transpose them to have the batch dimension in first place again.\n",
        "        outputs = tf.transpose(outputs.stack(), perm=[1,0,2])\n",
        "        alphas = tf.transpose(alphas.stack(), perm=[1,0,2])\n",
        "        inputs = tf.transpose(inputs.stack(), perm=[1,0])\n",
        "        outputs._keras_mask = (inputs != 0)\n",
        "\n",
        "        return outputs, alphas\n",
        "\n",
        "    def step(self, encoder_output, hidden_state, prev_embed, mask=None):\n",
        "        # 1. Get the attention weights and context vector\n",
        "        alpha, context = self.attention(hidden_state, encoder_output, mask=mask)\n",
        "        # 2. Concatenate the inputs for the GRU\n",
        "        rnn_input = tf.concat([prev_embed, context], axis=-1)\n",
        "        rnn_input = tf.expand_dims(rnn_input, axis=1)  # Add dimension\n",
        "        # 3. Take one step with the GRU cell\n",
        "        rnn_output, hidden_state = self.rnn_cell(rnn_input, initial_state=hidden_state)\n",
        "        rnn_output = tf.squeeze(rnn_output, axis=1)  # Remove dimension\n",
        "        # 4. Concatenate the respective tensors to produce the final output\n",
        "        output = tf.concat([rnn_output, context], axis=-1)\n",
        "        output = self.linear(output)\n",
        "\n",
        "        return output, hidden_state, alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej2-lzyVCutH"
      },
      "source": [
        "Your implementation should comply with the following specification:\n",
        "\n",
        "**step** (*self*, *encoder_output*, *hidden*, *prev_embed*, *mask*)\n",
        "\n",
        "> Performs a single step in the manual unrolling of the decoder GRU. This takes the output of the encoder (*encoder_output*), the previous hidden state of the decoder (*hidden*), the embedding vector of the previous word (*prev_embed*), and the source mask as described in Problem&nbsp;2.2 (*mask*), and computes the output as described above.\n",
        ">\n",
        "> The shape of *encoder_output* is (*batch_size*, *src_len*, *hidden_dim*); the shape of *hidden* is (*batch_size*, *hidden_dim*); the shape of *src_mask* is (*batch_size*, *src_len*); and the shape of *prev_embed* is (*batch_size*, *embedding_dim*).\n",
        ">\n",
        "> The method returns a triple of tensors (*output*, *hidden*, *alpha*) where *output* is the position-specific output of the GRU, of shape (*batch_size*, *num_words*); *hidden* is the new hidden state, of shape (*batch_size*, *hidden_dim*); and *alpha* are the attention weights that were used to compute the *output*, of shape (*batch_size*, *src_len*).\n",
        "\n",
        "#### 💡 Hints on the implementation\n",
        "\n",
        "**GRU vs. GRUCell.** In Keras, an RNN layer like `GRU` is used to process an entire sequence. A single *time-step* of a sequence is handled by a [`GRUCell`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell) instead. You can think of a `GRU` layer as functionally equivalent to a for-loop around a `GRUCell`. Since we want to perform the RNN steps individually for this model, you should use a [`GRUCell`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell) instead of a `GRU` layer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFvNiX-YCutI"
      },
      "source": [
        "#### 🤞 Test your code\n",
        "\n",
        "To test your code, extend your test from the previous problems, and simulate a complete forward pass of the encoder–decoder architecture on the example sentence. Check the shapes of the resulting tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "65lLG-bBCutI",
        "outputId": "5dc147a7-cc68-4c6f-b3cf-d62c87dfc74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 16, 5000)\n",
            "(4, 16, 5)\n",
            "(4, 8, 5000)\n"
          ]
        }
      ],
      "source": [
        "def test23():\n",
        "    src, tgt = list(train_dataset.take(4).padded_batch(4))[0]\n",
        "    encoder = Encoder(src_lookup.vocabulary_size())\n",
        "    encoder_output, hidden = encoder(src)\n",
        "    attention = BahdanauAttention()\n",
        "    decoder = Decoder(tgt_lookup, attention)\n",
        "    decoded, alphas = decoder(encoder_output, hidden)\n",
        "    print(decoded.shape)  # should be (batch_size, max_len, vocabulary_size)\n",
        "    print(alphas.shape)   # should be (batch_size, max_len, src_len)\n",
        "    decoded, _ = decoder(encoder_output, hidden, targets=tgt, training=True)\n",
        "    print(decoded.shape)  # should be (batch_size, tgt_len, vocabulary_size)\n",
        "\n",
        "test23()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70yHTFS9CutJ"
      },
      "source": [
        "### Encoder–Decoder wrapper class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeKfvES5CutJ"
      },
      "source": [
        "The last part of the implementation is a class that wraps the encoder and the decoder as a single model.  We also implement a custom `train_step` function so that the gold targets will get passed to the decoder during training, and a custom `test_step` function to make sure the decoded sequences and the gold sequences are padded to the same length before computing losses and evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "lUjIGniZCutJ"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(keras.Model):\n",
        "    def __init__(self, src_lookup, tgt_lookup, embedding_dim=128, hidden_dim=256, max_len=16, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = Encoder(\n",
        "            src_lookup.vocabulary_size(),\n",
        "            embedding_dim=embedding_dim,\n",
        "            hidden_dim=hidden_dim\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            tgt_lookup,\n",
        "            BahdanauAttention(hidden_dim=hidden_dim),\n",
        "            embedding_dim=embedding_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            max_len=max_len,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, training=False, targets=None):\n",
        "        x_out, x_hidden = self.encoder(inputs, training=training)\n",
        "        outputs, alphas = self.decoder(x_out, x_hidden, training=training, targets=targets)\n",
        "        if training:\n",
        "            return outputs\n",
        "        else:\n",
        "            return outputs, alphas\n",
        "\n",
        "    # Following <https://keras.io/guides/customizing_what_happens_in_fit/>\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Here we supply \"targets\" so that the decoder has access to it\n",
        "            y_pred = self(x, training=True, targets=y)\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "        y_pred, _alphas = self(x, training=False)\n",
        "\n",
        "        # Pad sequences to the same number of time-steps\n",
        "        max_len = tf.math.maximum(tf.shape(y)[1], tf.shape(y_pred)[1])\n",
        "        y_pad = [[0, 0], [0, max_len - tf.shape(y)[1]]]\n",
        "        y = tf.pad(y, y_pad)\n",
        "        y_pred_pad = [[0, 0], [0, max_len - tf.shape(y_pred)[1]], [0, 0]]\n",
        "        y_pred = tf.pad(y_pred, y_pred_pad)\n",
        "\n",
        "        self.compute_loss(x, y, y_pred, None)\n",
        "        return self.compute_metrics(x, y, y_pred, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3uW3TKACutK"
      },
      "source": [
        "#### 🤞 Test your code\n",
        "\n",
        "As a final test, instantiate an encoder–decoder model and use it to decode the example sentence. Check the shapes of the resulting tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oO6zONDwCutK",
        "outputId": "111dc6a5-683c-4e2e-a129-27c462e15a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 16, 5000)\n",
            "(1, 16, 6)\n"
          ]
        }
      ],
      "source": [
        "def test24():\n",
        "    src, tgt = example(train_dataset, 42)\n",
        "    encoder_decoder = EncoderDecoder(src_lookup, tgt_lookup)\n",
        "    outputs, alphas = encoder_decoder(src)\n",
        "    print(outputs.shape)  # should be (batch_size, max_len, vocabulary_size)\n",
        "    print(alphas.shape)   # should be (batch_size, max_len, src_len)\n",
        "\n",
        "test24()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laK9bbq8CutL"
      },
      "source": [
        "## Part 3: Train a translator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q68aWc_2CutL"
      },
      "source": [
        "We now have all the pieces to build and train a complete translation system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5MYw2PJCutL"
      },
      "source": [
        "### Translator class\n",
        "\n",
        "We first define a class `Translator` that initialises an encoder–decoder model and uses it to translate sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "Evi-J7BfCutM"
      },
      "outputs": [],
      "source": [
        "class Translator:\n",
        "    def __init__(self, src_lookup, tgt_lookup, batch_size=32, **kwargs):\n",
        "        self.src_lookup = src_lookup\n",
        "        self.tgt_lookup = tgt_lookup\n",
        "        self.model = EncoderDecoder(src_lookup, tgt_lookup, **kwargs)\n",
        "        self.tgt_vocab = tgt_lookup.get_vocabulary()\n",
        "        self.eos_index = self.tgt_vocab.index(\"<eos>\")\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def compile(self, *args, **kwargs):\n",
        "        return self.model.compile(*args, **kwargs)\n",
        "\n",
        "    def fit(self, *args, **kwargs):\n",
        "        return self.model.fit(*args, **kwargs)\n",
        "\n",
        "    def translate(self, sentences, return_alphas=False):\n",
        "        \"\"\"This function takes sentences and returns their translation as a string.\n",
        "\n",
        "        `sentences` can be either:\n",
        "          - A tf.data.Dataset object\n",
        "          - A list of strings\n",
        "        \"\"\"\n",
        "        if isinstance(sentences, tf.data.Dataset):\n",
        "            inputs = sentences\n",
        "        elif isinstance(sentences, (list, tuple)):\n",
        "            inputs = (\n",
        "                tf.data.Dataset.from_tensor_slices(\n",
        "                    tf.ragged.constant([x.split() for x in sentences])\n",
        "                )\n",
        "                .map(self.src_lookup)\n",
        "                .padded_batch(min(self.batch_size, len(sentences)))\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"'sentences' should be either a tf.Dataset or a list of strings; got: {type(sentences)}\")\n",
        "\n",
        "        outputs, alphas = self.model.predict(inputs, verbose=0)\n",
        "        outputs = tf.math.argmax(outputs, axis=-1).numpy().tolist()\n",
        "        try:\n",
        "            alphas = alphas.numpy().tolist()\n",
        "        except AttributeError:\n",
        "            alphas = alphas.tolist()\n",
        "        generated = []\n",
        "\n",
        "        for y_pred, alpha in zip(outputs, alphas):\n",
        "            try:\n",
        "                eos_idx = y_pred.index(self.eos_index)\n",
        "                del y_pred[eos_idx:]\n",
        "                del alpha[eos_idx:]\n",
        "            except ValueError:\n",
        "                pass\n",
        "            tokens = [self.tgt_vocab[idx] for idx in y_pred if idx > 0]\n",
        "            tokens = \" \".join(tokens)\n",
        "            if return_alphas:\n",
        "                generated.append((tokens, alpha))\n",
        "            else:\n",
        "                generated.append(tokens)\n",
        "\n",
        "        return generated\n",
        "\n",
        "    def translate_with_attention(self, sentences):\n",
        "        return self.translate(sentences, return_alphas=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2660iyt-CutN"
      },
      "source": [
        "The code below shows how this class is supposed to be used (its output will be nonsensical right now, of course, since the model hasn't been trained yet):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NFiJLKRXCutN",
        "outputId": "a1784f5b-215b-474f-d45d-4d7daafe202a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['attack control attack control control control anchovies control control control likes control control control anchovies control',\n",
              " 'lake absorbs smarter says says satisfactory ticklish waited unknown lake absorbs we smarter iq says says']"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ],
      "source": [
        "translator = Translator(src_lookup, tgt_lookup)\n",
        "# Alternative \"mini\" version of the model for testing:\n",
        "#translator = Translator(src_lookup, tgt_lookup, embedding_dim=32, hidden_dim=64, batch_size=16, max_len=8)\n",
        "translator.translate(['stäng av vattnet .', 'jag älskar friterade bananer .'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cFT9atlCutO"
      },
      "source": [
        "### Evaluation function\n",
        "\n",
        "As mentioned in the lecture, machine translation systems are typically evaluated using the **BLEU metric**. Here we use the implementation of this metric from the `sacrebleu` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d8rwXCWdVnTZ",
        "outputId": "e5e1ffbf-d531-41a2-9580-4d429ed66e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "PCDSAiLbTICs"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU\n",
        "bleu_params = dict(effective_order=True, tokenize=\"none\", force=True, smooth_method=\"floor\", smooth_value=0.01)\n",
        "bleu = BLEU(**bleu_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ja9YWTMTICv"
      },
      "source": [
        "If the test sentence is exactly identical to the reference sentence, the score will be 100 (plus/minus potential floating point rounding errors):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oq9sPRYjTICv",
        "outputId": "b11389b1-815c-4a33-ee8a-6e9a57b7ab6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100.00000000000004"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ],
      "source": [
        "bleu.sentence_score(\"the house is blue .\", [\"the house is blue .\"]).score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjjwdcL8TICv"
      },
      "source": [
        "If we change some words, the score will go down, though never below zero:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cMKQzL29TICv",
        "outputId": "0c7d7b26-c2f9-497a-d395-d1c53a11a31b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.9763536438352522"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ],
      "source": [
        "bleu.sentence_score(\"the house was red .\", [\"the house is blue .\"]).score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sBQBKg7TICv"
      },
      "source": [
        "Here is a helper function that takes a trained `Translator` model as well as a `Dataset`, runs all sentences through the translator, and computes the BLEU score for the entire dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "mgiyr_bzTICv"
      },
      "outputs": [],
      "source": [
        "def compute_bleu(translator, dataset):\n",
        "    hyp = translator.translate(dataset)\n",
        "    ref = [\n",
        "        \" \".join(translator.tgt_vocab[idx] for idx in s if idx not in (0, translator.eos_index))\n",
        "        for s in dataset.unbatch().map(lambda _, x: x).as_numpy_iterator()\n",
        "    ]\n",
        "    return bleu.corpus_score(hyp, [ref]).score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozk1hurqCutP"
      },
      "source": [
        "We want to report the BLEU score on the **validation data**, so let's load this as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "HNtBMDVqCutP"
      },
      "outputs": [],
      "source": [
        "valid_dataset = load_translation_dataset(src_lookup, tgt_lookup, \"en-sv-valid.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe2rPzwmCutQ"
      },
      "source": [
        "### Batching\n",
        "\n",
        "So far we only tested our code on \"batches\" with a single sentence. In order to use larger batches, we need to make sure that all of the sentences in a batch have the same length. We achieve this by _padding_ the shorter sentences to the length of the longest one. Luckily, the `tf.Dataset` class has a function [`padded_batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch) that will do this for us. If we provide a `Dataset` for training, Keras won't shuffle the data automatically, so we also have to [`shuffle`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) the dataset explicitly. (For validation, shuffling doesn't matter.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "X-yfmnSzCutQ"
      },
      "outputs": [],
      "source": [
        "train_batched = train_dataset.shuffle(512).padded_batch(64)\n",
        "valid_batched = valid_dataset.padded_batch(64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfM1Ov-sCutQ"
      },
      "source": [
        "### Training\n",
        "\n",
        "Training works as for any other Keras model: we first need to `compile` the model with the optimizer, loss function, and validation metrics that we want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "yr4VP-SpCutQ"
      },
      "outputs": [],
      "source": [
        "translator.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=2e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgXElolrCutR"
      },
      "source": [
        "Now it is time to train the system. During training, these diagnostics will be updated periodically: the running average of the training loss; after a full epoch, the loss and the BLEU score on the validation data will be computed and printed.\n",
        "\n",
        "Let's also define a callback that additionally prints the translation of a sample sentence, *jag saknar min familj* (which should translate into *i miss my family*), every 50 batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "2Bkypf-5TICx"
      },
      "outputs": [],
      "source": [
        "my_callbacks = [\n",
        "    keras.callbacks.LambdaCallback(\n",
        "        on_train_batch_end=lambda b, _: tf.print(\" - jag saknar min familj . ->\", translator.translate(['jag saknar min familj .'])[0]) if b > 0 and b % 50 == 0 else None,\n",
        "        on_epoch_end=lambda _, l: l.__setitem__(\"val_bleu\", compute_bleu(translator, valid_batched))\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3pLGb8BTICx"
      },
      "source": [
        "#### 🤔 Task 6: Run the model training\n",
        "\n",
        "Run the following code cells that train the model and evaluate it on the validation data.\n",
        "\n",
        "Training the translator takes quite a bit of compute power and time. The default number of epochs is 2; however, you may want to try training for longer, or interrupt the training prematurely and use a partially trained model in case you run out of time.\n",
        "\n",
        "**⚠️ Your submitted notebook must contain output demonstrating at least 20 BLEU points on the validation data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VW8wF8oRCutR",
        "outputId": "1aa896e4-e425-4164-d17e-6d55873b33ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            " 50/322 [===>..........................] - ETA: 1:26 - loss: 5.2698 - jag saknar min familj . -> .\n",
            "100/322 [========>.....................] - ETA: 1:14 - loss: 4.6824 - jag saknar min familj . -> the t t .\n",
            "150/322 [============>.................] - ETA: 49s - loss: 4.3316 - jag saknar min familj . -> i ' t do you the lot .\n",
            "200/322 [=================>............] - ETA: 33s - loss: 4.0800 - jag saknar min familj . -> i ' m not to go .\n",
            "250/322 [======================>.......] - ETA: 19s - loss: 3.8644 - jag saknar min familj . -> i ' m going to go .\n",
            "300/322 [==========================>...] - ETA: 5s - loss: 3.6818 - jag saknar min familj . -> i ' m a good .\n",
            "322/322 [==============================] - 108s 313ms/step - loss: 3.6119 - val_loss: 3.9604 - val_bleu: 6.4587\n",
            "Epoch 2/6\n",
            " 50/322 [===>..........................] - ETA: 1:02 - loss: 2.6606 - jag saknar min familj . -> i ' m glad i ' m going to the park .\n",
            "100/322 [========>.....................] - ETA: 48s - loss: 2.5870 - jag saknar min familj . -> i like it .\n",
            "150/322 [============>.................] - ETA: 37s - loss: 2.5076 - jag saknar min familj . -> i ' ve got to go .\n",
            "200/322 [=================>............] - ETA: 25s - loss: 2.4380 - jag saknar min familj . -> i think tom is stubborn .\n",
            "250/322 [======================>.......] - ETA: 15s - loss: 2.3615 - jag saknar min familj . -> i think my homework .\n",
            "300/322 [==========================>...] - ETA: 4s - loss: 2.2848 - jag saknar min familj . -> i miss my homework .\n",
            "322/322 [==============================] - 77s 239ms/step - loss: 2.2516 - val_loss: 4.0279 - val_bleu: 18.3058\n",
            "Epoch 3/6\n",
            " 50/322 [===>..........................] - ETA: 46s - loss: 1.8135 - jag saknar min familj . -> i think my parents ' s wife .\n",
            "100/322 [========>.....................] - ETA: 42s - loss: 1.7388 - jag saknar min familj . -> i think my bag .\n",
            "150/322 [============>.................] - ETA: 35s - loss: 1.6780 - jag saknar min familj . -> i miss my family .\n",
            "200/322 [=================>............] - ETA: 25s - loss: 1.6167 - jag saknar min familj . -> i miss my family .\n",
            "250/322 [======================>.......] - ETA: 15s - loss: 1.5530 - jag saknar min familj . -> i miss my family .\n",
            "300/322 [==========================>...] - ETA: 4s - loss: 1.4951 - jag saknar min familj . -> i miss my family .\n",
            "322/322 [==============================] - 75s 232ms/step - loss: 1.4701 - val_loss: 3.9674 - val_bleu: 31.8634\n",
            "Epoch 4/6\n",
            " 50/322 [===>..........................] - ETA: 1:17 - loss: 1.1161 - jag saknar min familj . -> i miss my wife .\n",
            "100/322 [========>.....................] - ETA: 58s - loss: 1.0566 - jag saknar min familj . -> i miss my family .\n",
            "150/322 [============>.................] - ETA: 43s - loss: 1.0242 - jag saknar min familj . -> i miss my family .\n",
            "200/322 [=================>............] - ETA: 30s - loss: 0.9877 - jag saknar min familj . -> i miss my family .\n",
            "250/322 [======================>.......] - ETA: 17s - loss: 0.9527 - jag saknar min familj . -> i miss my family .\n",
            "300/322 [==========================>...] - ETA: 5s - loss: 0.9180 - jag saknar min familj . -> i miss my family .\n",
            "322/322 [==============================] - 85s 265ms/step - loss: 0.9031 - val_loss: 3.8854 - val_bleu: 39.2333\n",
            "Epoch 5/6\n",
            " 50/322 [===>..........................] - ETA: 52s - loss: 0.6569 - jag saknar min familj . -> i miss my family .\n",
            "100/322 [========>.....................] - ETA: 42s - loss: 0.6300 - jag saknar min familj . -> i miss my family .\n",
            "150/322 [============>.................] - ETA: 36s - loss: 0.6174 - jag saknar min familj . -> i miss my family family .\n",
            "200/322 [=================>............] - ETA: 24s - loss: 0.6028 - jag saknar min familj . -> i miss my family .\n",
            "250/322 [======================>.......] - ETA: 15s - loss: 0.5867 - jag saknar min familj . -> i miss my family .\n",
            "300/322 [==========================>...] - ETA: 4s - loss: 0.5714 - jag saknar min familj . -> i miss my family .\n",
            "322/322 [==============================] - 73s 226ms/step - loss: 0.5646 - val_loss: 4.2230 - val_bleu: 43.8226\n",
            "Epoch 6/6\n",
            " 50/322 [===>..........................] - ETA: 43s - loss: 0.4137 - jag saknar min familj . -> i miss my family .\n",
            "100/322 [========>.....................] - ETA: 40s - loss: 0.4027 - jag saknar min familj . -> i miss my family .\n",
            "150/322 [============>.................] - ETA: 31s - loss: 0.4013 - jag saknar min familj . -> i miss my family .\n",
            "200/322 [=================>............] - ETA: 22s - loss: 0.3959 - jag saknar min familj . -> i miss my family .\n",
            "250/322 [======================>.......] - ETA: 13s - loss: 0.3895 - jag saknar min familj . -> i miss my family .\n",
            "300/322 [==========================>...] - ETA: 4s - loss: 0.3826 - jag saknar min familj . -> i miss my family .\n",
            "322/322 [==============================] - 69s 214ms/step - loss: 0.3807 - val_loss: 4.4559 - val_bleu: 45.4046\n",
            "CPU times: user 10min 17s, sys: 1min 26s, total: 11min 44s\n",
            "Wall time: 8min 7s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "try:\n",
        "    translator.fit(train_batched, epochs=6, validation_data=valid_batched, callbacks=my_callbacks)\n",
        "except KeyboardInterrupt:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eDqVL8s8TICx",
        "outputId": "884e964c-8486-484c-8580-811db029fed9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45.40457922794815"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ],
      "source": [
        "compute_bleu(translator, valid_batched)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p_QmDsETICy"
      },
      "source": [
        "#### ℹ️ Some notes on the translations\n",
        "\n",
        "If you try out sentences to see their translation (like in the code cell below), you might find some possibly surprising results, such as:\n",
        "\n",
        "- **Translations that are seemingly nonsensical or have nothing to do with the input.** This might be because the model is undertrained; you could try training for more epochs to see if the translations improve. It's also possible that you tried words or phrases that were just not well-represented in the training data.\n",
        "- **Translations that have a lot of `<unk>`s.** This might be due to the words just not being present in the model's vocabulary! Remember you can check this with the vocabularies you created, e.g. `\"friterade\" in src_vocab`. You could try increasing the vocabulary size and see if the results improve, but this will also increase training time.\n",
        "\n",
        "Finally, keep in mind that both the dataset and the model itself is quite tiny, as it's optimized for speed and demonstration purposes rather than efficiency!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "D_tsyZAgTICy",
        "outputId": "6bc962bd-6b70-4631-e922-635b03479c67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i miss my family .', 'close the the light .', 'i love <unk> bananas .']"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ],
      "source": [
        "translator.translate(['jag saknar min familj . ', 'stäng av vattnet .', 'jag älskar friterade bananer .'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7677Q0QLCutS"
      },
      "source": [
        "# Part 4: Visualising attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCzHCB4XCutS"
      },
      "source": [
        "Figure&nbsp;3 in the paper by [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473) shows some heatmaps of attention weights in selected sentences. In the last problem of this lab, we ask you to inspect attention weights for your trained translation system. We define a function `plot_attention` that visualises the attention weights. The *x*-axis corresponds to the words in the source sentence (Swedish) and the *y*-axis to the generated target sentence (English).\n",
        "\n",
        "The heatmap colours represent the **strengths of the attention weights**, with _lighter_ cells indicating a _higher_ attention value, just as in the Bahdanau et al. paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "IXHPiuamCutS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "def plot_attention(translator, sentence):\n",
        "    translation, weights = translator.translate_with_attention([sentence])[0]\n",
        "    weights = np.array(weights)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    heatmap = ax.pcolor(weights, cmap='Blues_r', vmin=0., vmax=1.)\n",
        "\n",
        "    ax.xaxis.tick_top()\n",
        "    ax.set_xticks(np.arange(weights.shape[1]) + 0.5, minor=False)\n",
        "    ax.set_yticks(np.arange(weights.shape[0]) + 0.5, minor=False)\n",
        "    ax.set_xticklabels(sentence.split(), minor=False, rotation=40)\n",
        "    ax.set_yticklabels(translation.split(), minor=False)\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    plt.colorbar(heatmap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGCwchIdCutS"
      },
      "source": [
        "Here is an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "Hj4teeITCutT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "22a01cc5-61ec-4b70-8998-fb724655c587"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"378.2701pt\" height=\"316.143071pt\" viewBox=\"0 0 378.2701 316.143071\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-05-14T22:41:06.139631</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 316.143071 \nL 378.2701 316.143071 \nL 378.2701 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.8125 303.244634 \nL 322.5085 303.244634 \nL 322.5085 37.132634 \nL 36.8125 37.132634 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 65.3821 303.244634 \nL 65.3821 37.132634 \n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_2\"/>\n     <g id=\"text_1\">\n      <!-- jag -->\n      <g style=\"fill: #262626\" transform=\"translate(61.933356 32.043091) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-6a\" d=\"M 428 4100 \nL 428 4638 \nL 991 4638 \nL 991 4100 \nL 428 4100 \nz\nM 991 -419 \nQ 991 -613 958 -778 \nQ 925 -944 840 -1066 \nQ 756 -1188 611 -1258 \nQ 466 -1328 241 -1328 \nQ 134 -1328 32 -1322 \nQ -69 -1316 -156 -1300 \nL -156 -866 \nQ -116 -872 -59 -878 \nQ -3 -884 38 -884 \nQ 156 -884 232 -853 \nQ 309 -822 353 -755 \nQ 397 -688 412 -583 \nQ 428 -478 428 -334 \nL 428 3381 \nL 991 3381 \nL 991 -419 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-61\" d=\"M 1294 -63 \nQ 784 -63 528 206 \nQ 272 475 272 944 \nQ 272 1278 398 1492 \nQ 525 1706 729 1828 \nQ 934 1950 1196 1997 \nQ 1459 2044 1731 2050 \nL 2491 2063 \nL 2491 2247 \nQ 2491 2456 2447 2603 \nQ 2403 2750 2312 2840 \nQ 2222 2931 2086 2973 \nQ 1950 3016 1766 3016 \nQ 1603 3016 1472 2992 \nQ 1341 2969 1244 2908 \nQ 1147 2847 1087 2742 \nQ 1028 2638 1009 2478 \nL 422 2531 \nQ 453 2731 540 2898 \nQ 628 3066 789 3187 \nQ 950 3309 1192 3376 \nQ 1434 3444 1778 3444 \nQ 2416 3444 2737 3151 \nQ 3059 2859 3059 2306 \nL 3059 850 \nQ 3059 600 3125 473 \nQ 3191 347 3375 347 \nQ 3422 347 3469 353 \nQ 3516 359 3559 369 \nL 3559 19 \nQ 3453 -6 3348 -18 \nQ 3244 -31 3125 -31 \nQ 2966 -31 2852 11 \nQ 2738 53 2666 139 \nQ 2594 225 2556 351 \nQ 2519 478 2509 647 \nL 2491 647 \nQ 2400 484 2292 353 \nQ 2184 222 2040 130 \nQ 1897 38 1714 -12 \nQ 1531 -63 1294 -63 \nz\nM 1422 359 \nQ 1691 359 1892 457 \nQ 2094 556 2226 709 \nQ 2359 863 2425 1044 \nQ 2491 1225 2491 1391 \nL 2491 1669 \nL 1875 1656 \nQ 1669 1653 1483 1626 \nQ 1297 1600 1156 1522 \nQ 1016 1444 933 1303 \nQ 850 1163 850 934 \nQ 850 659 998 509 \nQ 1147 359 1422 359 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-67\" d=\"M 1713 -1328 \nQ 1422 -1328 1197 -1268 \nQ 972 -1209 812 -1098 \nQ 653 -988 553 -834 \nQ 453 -681 409 -494 \nL 975 -413 \nQ 1031 -647 1223 -773 \nQ 1416 -900 1728 -900 \nQ 1919 -900 2075 -847 \nQ 2231 -794 2340 -676 \nQ 2450 -559 2509 -371 \nQ 2569 -184 2569 84 \nL 2569 628 \nL 2563 628 \nQ 2500 500 2408 383 \nQ 2316 266 2183 175 \nQ 2050 84 1875 29 \nQ 1700 -25 1475 -25 \nQ 1153 -25 923 83 \nQ 694 191 548 406 \nQ 403 622 336 942 \nQ 269 1263 269 1684 \nQ 269 2091 336 2416 \nQ 403 2741 554 2967 \nQ 706 3194 948 3314 \nQ 1191 3434 1538 3434 \nQ 1897 3434 2161 3270 \nQ 2425 3106 2569 2803 \nL 2575 2803 \nQ 2575 2881 2579 2978 \nQ 2584 3075 2589 3161 \nQ 2594 3247 2600 3309 \nQ 2606 3372 2613 3381 \nL 3147 3381 \nQ 3144 3353 3141 3286 \nQ 3138 3219 3134 3125 \nQ 3131 3031 3129 2917 \nQ 3128 2803 3128 2681 \nL 3128 97 \nQ 3128 -613 2779 -970 \nQ 2431 -1328 1713 -1328 \nz\nM 2569 1691 \nQ 2569 2044 2492 2295 \nQ 2416 2547 2291 2706 \nQ 2166 2866 2005 2941 \nQ 1844 3016 1675 3016 \nQ 1459 3016 1304 2941 \nQ 1150 2866 1048 2705 \nQ 947 2544 898 2292 \nQ 850 2041 850 1691 \nQ 850 1325 898 1076 \nQ 947 828 1047 675 \nQ 1147 522 1300 456 \nQ 1453 391 1666 391 \nQ 1834 391 1995 462 \nQ 2156 534 2284 690 \nQ 2413 847 2491 1094 \nQ 2569 1341 2569 1691 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-6a\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"22.216797\"/>\n       <use xlink:href=\"#LiberationSans-67\" x=\"77.832031\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 122.5213 303.244634 \nL 122.5213 37.132634 \n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_4\"/>\n     <g id=\"text_2\">\n      <!-- saknar -->\n      <g style=\"fill: #262626\" transform=\"translate(112.688054 32.043091) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-73\" d=\"M 2969 934 \nQ 2969 697 2876 511 \nQ 2784 325 2609 198 \nQ 2434 72 2179 4 \nQ 1925 -63 1597 -63 \nQ 1303 -63 1067 -17 \nQ 831 28 653 128 \nQ 475 228 354 392 \nQ 234 556 178 794 \nL 675 891 \nQ 747 619 972 492 \nQ 1197 366 1597 366 \nQ 1778 366 1929 391 \nQ 2081 416 2190 477 \nQ 2300 538 2361 639 \nQ 2422 741 2422 891 \nQ 2422 1044 2350 1142 \nQ 2278 1241 2150 1306 \nQ 2022 1372 1839 1420 \nQ 1656 1469 1438 1528 \nQ 1234 1581 1034 1647 \nQ 834 1713 673 1820 \nQ 513 1928 413 2087 \nQ 313 2247 313 2488 \nQ 313 2950 642 3192 \nQ 972 3434 1603 3434 \nQ 2163 3434 2492 3237 \nQ 2822 3041 2909 2606 \nL 2403 2544 \nQ 2375 2675 2300 2764 \nQ 2225 2853 2119 2908 \nQ 2013 2963 1880 2986 \nQ 1747 3009 1603 3009 \nQ 1222 3009 1040 2893 \nQ 859 2778 859 2544 \nQ 859 2406 926 2317 \nQ 994 2228 1114 2167 \nQ 1234 2106 1403 2061 \nQ 1572 2016 1775 1966 \nQ 1909 1931 2050 1892 \nQ 2191 1853 2323 1798 \nQ 2456 1744 2573 1670 \nQ 2691 1597 2778 1494 \nQ 2866 1391 2917 1253 \nQ 2969 1116 2969 934 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6b\" d=\"M 2550 0 \nL 1406 1544 \nL 994 1203 \nL 994 0 \nL 431 0 \nL 431 4638 \nL 994 4638 \nL 994 1741 \nL 2478 3381 \nL 3138 3381 \nL 1766 1928 \nL 3209 0 \nL 2550 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6e\" d=\"M 2578 0 \nL 2578 2144 \nQ 2578 2391 2542 2556 \nQ 2506 2722 2425 2823 \nQ 2344 2925 2211 2967 \nQ 2078 3009 1881 3009 \nQ 1681 3009 1520 2939 \nQ 1359 2869 1245 2736 \nQ 1131 2603 1068 2408 \nQ 1006 2213 1006 1959 \nL 1006 0 \nL 444 0 \nL 444 2659 \nQ 444 2766 442 2883 \nQ 441 3000 437 3104 \nQ 434 3209 431 3284 \nQ 428 3359 425 3381 \nL 956 3381 \nQ 959 3366 962 3297 \nQ 966 3228 970 3139 \nQ 975 3050 978 2958 \nQ 981 2866 981 2803 \nL 991 2803 \nQ 1072 2950 1169 3069 \nQ 1266 3188 1394 3270 \nQ 1522 3353 1687 3398 \nQ 1853 3444 2072 3444 \nQ 2353 3444 2556 3375 \nQ 2759 3306 2890 3162 \nQ 3022 3019 3083 2792 \nQ 3144 2566 3144 2253 \nL 3144 0 \nL 2578 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-72\" d=\"M 444 0 \nL 444 2594 \nQ 444 2700 442 2811 \nQ 441 2922 437 3025 \nQ 434 3128 431 3218 \nQ 428 3309 425 3381 \nL 956 3381 \nQ 959 3309 964 3217 \nQ 969 3125 973 3028 \nQ 978 2931 979 2842 \nQ 981 2753 981 2691 \nL 994 2691 \nQ 1053 2884 1120 3026 \nQ 1188 3169 1278 3261 \nQ 1369 3353 1494 3398 \nQ 1619 3444 1797 3444 \nQ 1866 3444 1928 3433 \nQ 1991 3422 2025 3413 \nL 2025 2897 \nQ 1969 2913 1894 2920 \nQ 1819 2928 1725 2928 \nQ 1531 2928 1395 2840 \nQ 1259 2753 1173 2598 \nQ 1088 2444 1047 2230 \nQ 1006 2016 1006 1763 \nL 1006 0 \nL 444 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-73\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"50\"/>\n       <use xlink:href=\"#LiberationSans-6b\" x=\"105.615234\"/>\n       <use xlink:href=\"#LiberationSans-6e\" x=\"155.615234\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"211.230469\"/>\n       <use xlink:href=\"#LiberationSans-72\" x=\"266.845703\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 179.6605 303.244634 \nL 179.6605 37.132634 \n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_6\"/>\n     <g id=\"text_3\">\n      <!-- min -->\n      <g style=\"fill: #262626\" transform=\"translate(175.151263 32.043091) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-6d\" d=\"M 2400 0 \nL 2400 2144 \nQ 2400 2391 2369 2556 \nQ 2338 2722 2264 2823 \nQ 2191 2925 2072 2967 \nQ 1953 3009 1781 3009 \nQ 1603 3009 1459 2939 \nQ 1316 2869 1214 2736 \nQ 1113 2603 1058 2408 \nQ 1003 2213 1003 1959 \nL 1003 0 \nL 444 0 \nL 444 2659 \nQ 444 2766 442 2883 \nQ 441 3000 437 3104 \nQ 434 3209 431 3284 \nQ 428 3359 425 3381 \nL 956 3381 \nQ 959 3366 962 3297 \nQ 966 3228 970 3139 \nQ 975 3050 978 2958 \nQ 981 2866 981 2803 \nL 991 2803 \nQ 1066 2950 1153 3069 \nQ 1241 3188 1358 3270 \nQ 1475 3353 1626 3398 \nQ 1778 3444 1978 3444 \nQ 2363 3444 2586 3291 \nQ 2809 3138 2897 2803 \nL 2906 2803 \nQ 2981 2950 3075 3069 \nQ 3169 3188 3294 3270 \nQ 3419 3353 3575 3398 \nQ 3731 3444 3931 3444 \nQ 4188 3444 4373 3375 \nQ 4559 3306 4678 3162 \nQ 4797 3019 4853 2792 \nQ 4909 2566 4909 2253 \nL 4909 0 \nL 4353 0 \nL 4353 2144 \nQ 4353 2391 4322 2556 \nQ 4291 2722 4217 2823 \nQ 4144 2925 4025 2967 \nQ 3906 3009 3734 3009 \nQ 3556 3009 3412 2942 \nQ 3269 2875 3167 2744 \nQ 3066 2613 3011 2416 \nQ 2956 2219 2956 1959 \nL 2956 0 \nL 2400 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-69\" d=\"M 428 4100 \nL 428 4638 \nL 991 4638 \nL 991 4100 \nL 428 4100 \nz\nM 428 0 \nL 428 3381 \nL 991 3381 \nL 991 0 \nL 428 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-6d\"/>\n       <use xlink:href=\"#LiberationSans-69\" x=\"83.300781\"/>\n       <use xlink:href=\"#LiberationSans-6e\" x=\"105.517578\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 236.7997 303.244634 \nL 236.7997 37.132634 \n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_8\"/>\n     <g id=\"text_4\">\n      <!-- familj -->\n      <g style=\"fill: #262626\" transform=\"translate(229.524324 32.043091) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-66\" d=\"M 1128 2972 \nL 1128 0 \nL 566 0 \nL 566 2972 \nL 91 2972 \nL 91 3381 \nL 566 3381 \nL 566 3763 \nQ 566 3947 600 4105 \nQ 634 4263 726 4380 \nQ 819 4497 978 4564 \nQ 1138 4631 1391 4631 \nQ 1491 4631 1598 4622 \nQ 1706 4613 1788 4594 \nL 1788 4166 \nQ 1734 4175 1664 4183 \nQ 1594 4191 1538 4191 \nQ 1413 4191 1333 4156 \nQ 1253 4122 1208 4058 \nQ 1163 3994 1145 3900 \nQ 1128 3806 1128 3684 \nL 1128 3381 \nL 1788 3381 \nL 1788 2972 \nL 1128 2972 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6c\" d=\"M 431 0 \nL 431 4638 \nL 994 4638 \nL 994 0 \nL 431 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-66\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"27.783203\"/>\n       <use xlink:href=\"#LiberationSans-6d\" x=\"83.398438\"/>\n       <use xlink:href=\"#LiberationSans-69\" x=\"166.699219\"/>\n       <use xlink:href=\"#LiberationSans-6c\" x=\"188.916016\"/>\n       <use xlink:href=\"#LiberationSans-6a\" x=\"211.132812\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 293.9389 303.244634 \nL 293.9389 37.132634 \n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_10\"/>\n     <g id=\"text_5\">\n      <!-- . -->\n      <g style=\"fill: #262626\" transform=\"translate(294.537025 32.043091) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-2e\" d=\"M 584 0 \nL 584 684 \nL 1194 684 \nL 1194 0 \nL 584 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-2e\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path d=\"M 36.8125 63.743834 \nL 322.5085 63.743834 \n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_12\"/>\n     <g id=\"text_6\">\n      <!-- i -->\n      <g style=\"fill: #262626\" transform=\"translate(31.090625 67.367271) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-69\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path d=\"M 36.8125 116.966234 \nL 322.5085 116.966234 \n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_14\"/>\n     <g id=\"text_7\">\n      <!-- miss -->\n      <g style=\"fill: #262626\" transform=\"translate(12.760938 120.589671) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-6d\"/>\n       <use xlink:href=\"#LiberationSans-69\" x=\"83.300781\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"105.517578\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"155.517578\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path d=\"M 36.8125 170.188634 \nL 322.5085 170.188634 \n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_16\"/>\n     <g id=\"text_8\">\n      <!-- my -->\n      <g style=\"fill: #262626\" transform=\"translate(19.982813 173.812071) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-79\" d=\"M 1888 0 \nQ 1769 -306 1645 -551 \nQ 1522 -797 1369 -970 \nQ 1216 -1144 1028 -1236 \nQ 841 -1328 597 -1328 \nQ 491 -1328 400 -1322 \nQ 309 -1316 209 -1294 \nL 209 -872 \nQ 269 -881 344 -886 \nQ 419 -891 472 -891 \nQ 719 -891 931 -706 \nQ 1144 -522 1303 -119 \nL 1356 16 \nL 16 3381 \nL 616 3381 \nL 1328 1513 \nQ 1359 1428 1407 1287 \nQ 1456 1147 1504 1006 \nQ 1553 866 1590 753 \nQ 1628 641 1634 613 \nQ 1644 647 1680 748 \nQ 1716 850 1761 975 \nQ 1806 1100 1853 1228 \nQ 1900 1356 1931 1450 \nL 2594 3381 \nL 3188 3381 \nL 1888 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-6d\"/>\n       <use xlink:href=\"#LiberationSans-79\" x=\"83.300781\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path d=\"M 36.8125 223.411034 \nL 322.5085 223.411034 \n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_18\"/>\n     <g id=\"text_9\">\n      <!-- family -->\n      <g style=\"fill: #262626\" transform=\"translate(7.2 227.034471) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-66\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"27.783203\"/>\n       <use xlink:href=\"#LiberationSans-6d\" x=\"83.398438\"/>\n       <use xlink:href=\"#LiberationSans-69\" x=\"166.699219\"/>\n       <use xlink:href=\"#LiberationSans-6c\" x=\"188.916016\"/>\n       <use xlink:href=\"#LiberationSans-79\" x=\"211.132812\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path d=\"M 36.8125 276.633434 \nL 322.5085 276.633434 \n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_20\"/>\n     <g id=\"text_10\">\n      <!-- . -->\n      <g style=\"fill: #262626\" transform=\"translate(30.534375 280.256871) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-2e\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"PolyCollection_1\">\n    <path d=\"M 36.8125 37.132634 \nL 36.8125 90.355034 \nL 93.9517 90.355034 \nL 93.9517 37.132634 \nL 36.8125 37.132634 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #64a9d3\"/>\n    <path d=\"M 93.9517 37.132634 \nL 93.9517 90.355034 \nL 151.0909 90.355034 \nL 151.0909 37.132634 \nL 93.9517 37.132634 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #1e6db2\"/>\n    <path d=\"M 151.0909 37.132634 \nL 151.0909 90.355034 \nL 208.2301 90.355034 \nL 208.2301 37.132634 \nL 151.0909 37.132634 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #08509b\"/>\n    <path d=\"M 208.2301 37.132634 \nL 208.2301 90.355034 \nL 265.3693 90.355034 \nL 265.3693 37.132634 \nL 208.2301 37.132634 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #084d96\"/>\n    <path d=\"M 265.3693 37.132634 \nL 265.3693 90.355034 \nL 322.5085 90.355034 \nL 322.5085 37.132634 \nL 265.3693 37.132634 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #083d7f\"/>\n    <path d=\"M 36.8125 90.355034 \nL 36.8125 143.577434 \nL 93.9517 143.577434 \nL 93.9517 90.355034 \nL 36.8125 90.355034 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #083d7f\"/>\n    <path d=\"M 93.9517 90.355034 \nL 93.9517 143.577434 \nL 151.0909 143.577434 \nL 151.0909 90.355034 \nL 93.9517 90.355034 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #e1edf8\"/>\n    <path d=\"M 151.0909 90.355034 \nL 151.0909 143.577434 \nL 208.2301 143.577434 \nL 208.2301 90.355034 \nL 151.0909 90.355034 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #083a7a\"/>\n    <path d=\"M 208.2301 90.355034 \nL 208.2301 143.577434 \nL 265.3693 143.577434 \nL 265.3693 90.355034 \nL 208.2301 90.355034 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #083471\"/>\n    <path d=\"M 265.3693 90.355034 \nL 265.3693 143.577434 \nL 322.5085 143.577434 \nL 322.5085 90.355034 \nL 265.3693 90.355034 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #08306b\"/>\n    <path d=\"M 36.8125 143.577434 \nL 36.8125 196.799834 \nL 93.9517 196.799834 \nL 93.9517 143.577434 \nL 36.8125 143.577434 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #083573\"/>\n    <path d=\"M 93.9517 143.577434 \nL 93.9517 196.799834 \nL 151.0909 196.799834 \nL 151.0909 143.577434 \nL 93.9517 143.577434 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #0e58a2\"/>\n    <path d=\"M 151.0909 143.577434 \nL 151.0909 196.799834 \nL 208.2301 196.799834 \nL 208.2301 143.577434 \nL 151.0909 143.577434 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #a6cee4\"/>\n    <path d=\"M 208.2301 143.577434 \nL 208.2301 196.799834 \nL 265.3693 196.799834 \nL 265.3693 143.577434 \nL 208.2301 143.577434 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #0e59a2\"/>\n    <path d=\"M 265.3693 143.577434 \nL 265.3693 196.799834 \nL 322.5085 196.799834 \nL 322.5085 143.577434 \nL 265.3693 143.577434 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #083471\"/>\n    <path d=\"M 36.8125 196.799834 \nL 36.8125 250.022234 \nL 93.9517 250.022234 \nL 93.9517 196.799834 \nL 36.8125 196.799834 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #08306b\"/>\n    <path d=\"M 93.9517 196.799834 \nL 93.9517 250.022234 \nL 151.0909 250.022234 \nL 151.0909 196.799834 \nL 93.9517 196.799834 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #08316d\"/>\n    <path d=\"M 151.0909 196.799834 \nL 151.0909 250.022234 \nL 208.2301 250.022234 \nL 208.2301 196.799834 \nL 151.0909 196.799834 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #083c7d\"/>\n    <path d=\"M 208.2301 196.799834 \nL 208.2301 250.022234 \nL 265.3693 250.022234 \nL 265.3693 196.799834 \nL 208.2301 196.799834 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #e9f2fa\"/>\n    <path d=\"M 265.3693 196.799834 \nL 265.3693 250.022234 \nL 322.5085 250.022234 \nL 322.5085 196.799834 \nL 265.3693 196.799834 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #083573\"/>\n    <path d=\"M 36.8125 250.022234 \nL 36.8125 303.244634 \nL 93.9517 303.244634 \nL 93.9517 250.022234 \nL 36.8125 250.022234 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #08306b\"/>\n    <path d=\"M 93.9517 250.022234 \nL 93.9517 303.244634 \nL 151.0909 303.244634 \nL 151.0909 250.022234 \nL 93.9517 250.022234 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #08306b\"/>\n    <path d=\"M 151.0909 250.022234 \nL 151.0909 303.244634 \nL 208.2301 303.244634 \nL 208.2301 250.022234 \nL 151.0909 250.022234 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #083471\"/>\n    <path d=\"M 208.2301 250.022234 \nL 208.2301 303.244634 \nL 265.3693 303.244634 \nL 265.3693 250.022234 \nL 208.2301 250.022234 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #d3e3f3\"/>\n    <path d=\"M 265.3693 250.022234 \nL 265.3693 303.244634 \nL 322.5085 303.244634 \nL 322.5085 250.022234 \nL 265.3693 250.022234 \nz\n\" clip-path=\"url(#p01aaa73c21)\" style=\"fill: #105ba4\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.8125 303.244634 \nL 36.8125 37.132634 \n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 322.5085 303.244634 \nL 322.5085 37.132634 \n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.8125 303.244634 \nL 322.5085 303.244634 \n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.8125 37.132634 \nL 322.5085 37.132634 \n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 340.3645 303.244634 \nL 353.6701 303.244634 \nL 353.6701 37.132634 \nL 340.3645 37.132634 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_6\">\n     <g id=\"line2d_21\"/>\n     <g id=\"text_11\">\n      <!-- 0.0 -->\n      <g style=\"fill: #262626\" transform=\"translate(357.1701 306.868071) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-30\" d=\"M 3309 2203 \nQ 3309 1569 3189 1136 \nQ 3069 703 2861 436 \nQ 2653 169 2372 53 \nQ 2091 -63 1772 -63 \nQ 1450 -63 1172 53 \nQ 894 169 689 434 \nQ 484 700 367 1133 \nQ 250 1566 250 2203 \nQ 250 2869 367 3305 \nQ 484 3741 690 4000 \nQ 897 4259 1178 4364 \nQ 1459 4469 1791 4469 \nQ 2106 4469 2382 4364 \nQ 2659 4259 2865 4000 \nQ 3072 3741 3190 3305 \nQ 3309 2869 3309 2203 \nz\nM 2738 2203 \nQ 2738 2728 2675 3076 \nQ 2613 3425 2491 3633 \nQ 2369 3841 2192 3927 \nQ 2016 4013 1791 4013 \nQ 1553 4013 1372 3925 \nQ 1191 3838 1067 3630 \nQ 944 3422 881 3073 \nQ 819 2725 819 2203 \nQ 819 1697 883 1350 \nQ 947 1003 1070 792 \nQ 1194 581 1372 489 \nQ 1550 397 1778 397 \nQ 2000 397 2178 489 \nQ 2356 581 2479 792 \nQ 2603 1003 2670 1350 \nQ 2738 1697 2738 2203 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-30\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_22\"/>\n     <g id=\"text_12\">\n      <!-- 0.2 -->\n      <g style=\"fill: #262626\" transform=\"translate(357.1701 253.645671) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-32\" d=\"M 322 0 \nL 322 397 \nQ 481 763 711 1042 \nQ 941 1322 1194 1548 \nQ 1447 1775 1695 1969 \nQ 1944 2163 2144 2356 \nQ 2344 2550 2467 2762 \nQ 2591 2975 2591 3244 \nQ 2591 3431 2534 3573 \nQ 2478 3716 2372 3812 \nQ 2266 3909 2117 3957 \nQ 1969 4006 1788 4006 \nQ 1619 4006 1470 3959 \nQ 1322 3913 1206 3819 \nQ 1091 3725 1017 3586 \nQ 944 3447 922 3263 \nL 347 3316 \nQ 375 3553 478 3762 \nQ 581 3972 762 4130 \nQ 944 4288 1198 4378 \nQ 1453 4469 1788 4469 \nQ 2116 4469 2372 4391 \nQ 2628 4313 2804 4159 \nQ 2981 4006 3075 3781 \nQ 3169 3556 3169 3263 \nQ 3169 3041 3089 2841 \nQ 3009 2641 2876 2459 \nQ 2744 2278 2569 2109 \nQ 2394 1941 2203 1780 \nQ 2013 1619 1819 1461 \nQ 1625 1303 1454 1143 \nQ 1284 984 1150 820 \nQ 1016 656 941 478 \nL 3238 478 \nL 3238 0 \nL 322 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-32\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_23\"/>\n     <g id=\"text_13\">\n      <!-- 0.4 -->\n      <g style=\"fill: #262626\" transform=\"translate(357.1701 200.423271) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-34\" d=\"M 2753 997 \nL 2753 0 \nL 2222 0 \nL 2222 997 \nL 147 997 \nL 147 1434 \nL 2163 4403 \nL 2753 4403 \nL 2753 1441 \nL 3372 1441 \nL 3372 997 \nL 2753 997 \nz\nM 2222 3769 \nQ 2216 3753 2191 3708 \nQ 2166 3663 2134 3606 \nQ 2103 3550 2070 3492 \nQ 2038 3434 2013 3397 \nL 884 1734 \nQ 869 1709 839 1668 \nQ 809 1628 778 1586 \nQ 747 1544 715 1503 \nQ 684 1463 666 1441 \nL 2222 1441 \nL 2222 3769 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-34\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_24\"/>\n     <g id=\"text_14\">\n      <!-- 0.6 -->\n      <g style=\"fill: #262626\" transform=\"translate(357.1701 147.200871) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-36\" d=\"M 3278 1441 \nQ 3278 1109 3186 832 \nQ 3094 556 2914 357 \nQ 2734 159 2468 48 \nQ 2203 -63 1856 -63 \nQ 1472 -63 1184 84 \nQ 897 231 706 507 \nQ 516 784 420 1186 \nQ 325 1588 325 2100 \nQ 325 2688 433 3131 \nQ 541 3575 744 3872 \nQ 947 4169 1239 4319 \nQ 1531 4469 1900 4469 \nQ 2125 4469 2322 4422 \nQ 2519 4375 2680 4270 \nQ 2841 4166 2962 3994 \nQ 3084 3822 3156 3572 \nL 2619 3475 \nQ 2531 3759 2339 3886 \nQ 2147 4013 1894 4013 \nQ 1663 4013 1475 3903 \nQ 1288 3794 1156 3576 \nQ 1025 3359 954 3031 \nQ 884 2703 884 2266 \nQ 1038 2550 1316 2698 \nQ 1594 2847 1953 2847 \nQ 2253 2847 2497 2750 \nQ 2741 2653 2914 2470 \nQ 3088 2288 3183 2027 \nQ 3278 1766 3278 1441 \nz\nM 2706 1416 \nQ 2706 1644 2650 1828 \nQ 2594 2013 2481 2142 \nQ 2369 2272 2203 2342 \nQ 2038 2413 1819 2413 \nQ 1666 2413 1509 2367 \nQ 1353 2322 1226 2220 \nQ 1100 2119 1020 1953 \nQ 941 1788 941 1550 \nQ 941 1306 1003 1095 \nQ 1066 884 1183 728 \nQ 1300 572 1465 481 \nQ 1631 391 1838 391 \nQ 2041 391 2202 461 \nQ 2363 531 2475 664 \nQ 2588 797 2647 987 \nQ 2706 1178 2706 1416 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-36\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_25\"/>\n     <g id=\"text_15\">\n      <!-- 0.8 -->\n      <g style=\"fill: #262626\" transform=\"translate(357.1701 93.978471) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-38\" d=\"M 3281 1228 \nQ 3281 947 3192 711 \nQ 3103 475 2920 303 \nQ 2738 131 2453 34 \nQ 2169 -63 1781 -63 \nQ 1394 -63 1111 34 \nQ 828 131 642 301 \nQ 456 472 367 708 \nQ 278 944 278 1222 \nQ 278 1463 351 1650 \nQ 425 1838 548 1973 \nQ 672 2109 830 2192 \nQ 988 2275 1156 2303 \nL 1156 2316 \nQ 972 2359 826 2456 \nQ 681 2553 582 2689 \nQ 484 2825 432 2990 \nQ 381 3156 381 3341 \nQ 381 3572 470 3776 \nQ 559 3981 734 4136 \nQ 909 4291 1168 4380 \nQ 1428 4469 1769 4469 \nQ 2128 4469 2392 4378 \nQ 2656 4288 2829 4133 \nQ 3003 3978 3087 3772 \nQ 3172 3566 3172 3334 \nQ 3172 3153 3120 2987 \nQ 3069 2822 2970 2686 \nQ 2872 2550 2726 2454 \nQ 2581 2359 2391 2322 \nL 2391 2309 \nQ 2581 2278 2743 2195 \nQ 2906 2113 3025 1977 \nQ 3144 1841 3212 1653 \nQ 3281 1466 3281 1228 \nz\nM 2588 3303 \nQ 2588 3469 2545 3606 \nQ 2503 3744 2406 3842 \nQ 2309 3941 2153 3995 \nQ 1997 4050 1769 4050 \nQ 1547 4050 1394 3995 \nQ 1241 3941 1142 3842 \nQ 1044 3744 1000 3606 \nQ 956 3469 956 3303 \nQ 956 3172 990 3034 \nQ 1025 2897 1115 2784 \nQ 1206 2672 1365 2600 \nQ 1525 2528 1775 2528 \nQ 2041 2528 2202 2600 \nQ 2363 2672 2448 2784 \nQ 2534 2897 2561 3034 \nQ 2588 3172 2588 3303 \nz\nM 2697 1281 \nQ 2697 1441 2653 1589 \nQ 2609 1738 2503 1852 \nQ 2397 1966 2217 2036 \nQ 2038 2106 1769 2106 \nQ 1522 2106 1348 2036 \nQ 1175 1966 1067 1850 \nQ 959 1734 909 1582 \nQ 859 1431 859 1269 \nQ 859 1066 909 898 \nQ 959 731 1068 611 \nQ 1178 491 1356 425 \nQ 1534 359 1788 359 \nQ 2044 359 2219 425 \nQ 2394 491 2500 611 \nQ 2606 731 2651 901 \nQ 2697 1072 2697 1281 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-38\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_26\"/>\n     <g id=\"text_16\">\n      <!-- 1.0 -->\n      <g style=\"fill: #262626\" transform=\"translate(357.1701 40.756071) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-31\" d=\"M 488 0 \nL 488 478 \nL 1609 478 \nL 1609 3866 \nL 616 3156 \nL 616 3688 \nL 1656 4403 \nL 2175 4403 \nL 2175 478 \nL 3247 478 \nL 3247 0 \nL 488 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-31\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-30\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAABIAAAFyCAYAAAD4TYq5AAABwklEQVR4nO2cwW0EMQwD7cU+Uk76Sv/vSwnSY3AYEGQBxFKUKNvA3f35/fscAO+5D8FzGJZzznvupYgqbUCwNOyLyBpV2kwULE03tE+HdkClrYgqbUCwNGOw2WpUaTOSpV1q1jCiw/Ak16jSZmB9JJSmc01Yo9r/PaK6NsPnmrBGtf97RHVths81YY1q/4YI4am0BXzS3gd6zyZrhBFV2oDgzg6WprQfI6q0AZU2wyctO9hq/4BKmwGeaju0IxF08DdKE7oGPUXUtQURNf3CYGtmjyCHliLSSaNixDf9wmBLzmyEhu1sWUMKpz85s3Ndo5iM0riGhIiCa+TbtFhmU58kzKPgqAWnvzUa0CeNGd0iC6LgYNPViDxDym5HQtd8DambNaP9FJEvs7F3SGFnY0S6huSeDxGa7M72SatrCyLd0OrOR9g1y3gayZXm6+zg6zomLXhouTutThroGgPhw7hPGvfbLNv0Cw9awn+uqGsjUV1bEDE8ycHWTTujrm2IGJ4G2wJdRzPq2oKowbYgYni6jhZIdg2TdhpsM5FQGuSacdNSREJpMteSgy13HWFf5JMW7JrvnC0MNt2I1LUNkU3aP0nMCNDmnYQYAAAAAElFTkSuQmCC\" id=\"image03500bcc4f\" transform=\"scale(1 -1) translate(0 -266.4)\" x=\"340.56\" y=\"-36.72\" width=\"12.96\" height=\"266.4\"/>\n   <g id=\"LineCollection_1\"/>\n   <g id=\"patch_8\">\n    <path d=\"M 340.3645 303.244634 \nL 347.0173 303.244634 \nL 353.6701 303.244634 \nL 353.6701 37.132634 \nL 347.0173 37.132634 \nL 340.3645 37.132634 \nL 340.3645 303.244634 \nz\n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p01aaa73c21\">\n   <rect x=\"36.8125\" y=\"37.132634\" width=\"285.696\" height=\"266.112\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_attention(translator, 'jag saknar min familj . ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2xDJxW7TICz"
      },
      "source": [
        "#### 🤔 Task 7: Use these heatmaps to inspect the attention patterns for selected Swedish sentences\n",
        "\n",
        "Try to find sentences for which the model produces reasonably good English translations. If you don't speak Swedish, use sentences from the validation data. It might be interesting to look at examples where the Swedish and the English word order differ substantially.\n",
        "\n",
        "Based on your exploration, **answer the following questions:**\n",
        "\n",
        "- What sentences did you try out? What patterns did you spot? Include example heatmaps in your notebook.\n",
        "- Based on what you know about attention, did you expect your results? Was there anything surprising in them?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "swedish_sentence = 'tom är skyldig mig en massa pengar .'\n",
        "print(translator.translate([swedish_sentence]))\n",
        "plot_attention(translator, swedish_sentence)\n",
        "\n",
        "swedish_sentence = 'då kommer mormor att få hjälp .'\n",
        "print(translator.translate([swedish_sentence]))\n",
        "plot_attention(translator, swedish_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        },
        "id": "K5885oHwtSfG",
        "outputId": "286ec04e-5ac1-478b-ce32-a2208a99a5d3"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tom owes me a lot of money .']\n",
            "['time is the <unk> to help .']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"382.1701pt\" height=\"316.864199pt\" viewBox=\"0 0 382.1701 316.864199\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-05-14T22:50:04.073496</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 316.864199 \nL 382.1701 316.864199 \nL 382.1701 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.7125 303.965761 \nL 326.4085 303.965761 \nL 326.4085 37.853761 \nL 40.7125 37.853761 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 58.5685 303.965761 \nL 58.5685 37.853761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_2\"/>\n     <g id=\"text_1\">\n      <!-- tom -->\n      <g style=\"fill: #262626\" transform=\"translate(53.846207 32.764219) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-74\" d=\"M 1731 25 \nQ 1603 -9 1470 -29 \nQ 1338 -50 1163 -50 \nQ 488 -50 488 716 \nL 488 2972 \nL 97 2972 \nL 97 3381 \nL 509 3381 \nL 675 4138 \nL 1050 4138 \nL 1050 3381 \nL 1675 3381 \nL 1675 2972 \nL 1050 2972 \nL 1050 838 \nQ 1050 594 1129 495 \nQ 1209 397 1406 397 \nQ 1488 397 1564 409 \nQ 1641 422 1731 441 \nL 1731 25 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6f\" d=\"M 3291 1694 \nQ 3291 806 2900 371 \nQ 2509 -63 1766 -63 \nQ 1413 -63 1134 43 \nQ 856 150 664 369 \nQ 472 588 370 917 \nQ 269 1247 269 1694 \nQ 269 3444 1784 3444 \nQ 2178 3444 2464 3334 \nQ 2750 3225 2933 3006 \nQ 3116 2788 3203 2459 \nQ 3291 2131 3291 1694 \nz\nM 2700 1694 \nQ 2700 2088 2639 2344 \nQ 2578 2600 2461 2753 \nQ 2344 2906 2175 2967 \nQ 2006 3028 1794 3028 \nQ 1578 3028 1404 2964 \nQ 1231 2900 1109 2745 \nQ 988 2591 923 2334 \nQ 859 2078 859 1694 \nQ 859 1300 928 1042 \nQ 997 784 1117 631 \nQ 1238 478 1402 415 \nQ 1566 353 1759 353 \nQ 1975 353 2150 414 \nQ 2325 475 2447 628 \nQ 2569 781 2634 1040 \nQ 2700 1300 2700 1694 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6d\" d=\"M 2400 0 \nL 2400 2144 \nQ 2400 2391 2369 2556 \nQ 2338 2722 2264 2823 \nQ 2191 2925 2072 2967 \nQ 1953 3009 1781 3009 \nQ 1603 3009 1459 2939 \nQ 1316 2869 1214 2736 \nQ 1113 2603 1058 2408 \nQ 1003 2213 1003 1959 \nL 1003 0 \nL 444 0 \nL 444 2659 \nQ 444 2766 442 2883 \nQ 441 3000 437 3104 \nQ 434 3209 431 3284 \nQ 428 3359 425 3381 \nL 956 3381 \nQ 959 3366 962 3297 \nQ 966 3228 970 3139 \nQ 975 3050 978 2958 \nQ 981 2866 981 2803 \nL 991 2803 \nQ 1066 2950 1153 3069 \nQ 1241 3188 1358 3270 \nQ 1475 3353 1626 3398 \nQ 1778 3444 1978 3444 \nQ 2363 3444 2586 3291 \nQ 2809 3138 2897 2803 \nL 2906 2803 \nQ 2981 2950 3075 3069 \nQ 3169 3188 3294 3270 \nQ 3419 3353 3575 3398 \nQ 3731 3444 3931 3444 \nQ 4188 3444 4373 3375 \nQ 4559 3306 4678 3162 \nQ 4797 3019 4853 2792 \nQ 4909 2566 4909 2253 \nL 4909 0 \nL 4353 0 \nL 4353 2144 \nQ 4353 2391 4322 2556 \nQ 4291 2722 4217 2823 \nQ 4144 2925 4025 2967 \nQ 3906 3009 3734 3009 \nQ 3556 3009 3412 2942 \nQ 3269 2875 3167 2744 \nQ 3066 2613 3011 2416 \nQ 2956 2219 2956 1959 \nL 2956 0 \nL 2400 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-74\"/>\n       <use xlink:href=\"#LiberationSans-6f\" x=\"27.783203\"/>\n       <use xlink:href=\"#LiberationSans-6d\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 94.2805 303.965761 \nL 94.2805 37.853761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_4\"/>\n     <g id=\"text_2\">\n      <!-- är -->\n      <g style=\"fill: #262626\" transform=\"translate(92.537402 32.764219) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-e4\" d=\"M 1294 -63 \nQ 784 -63 528 206 \nQ 272 475 272 944 \nQ 272 1278 398 1492 \nQ 525 1706 729 1828 \nQ 934 1950 1196 1997 \nQ 1459 2044 1731 2050 \nL 2491 2063 \nL 2491 2247 \nQ 2491 2456 2447 2603 \nQ 2403 2750 2312 2840 \nQ 2222 2931 2086 2973 \nQ 1950 3016 1766 3016 \nQ 1603 3016 1472 2992 \nQ 1341 2969 1244 2908 \nQ 1147 2847 1087 2742 \nQ 1028 2638 1009 2478 \nL 422 2531 \nQ 453 2731 540 2898 \nQ 628 3066 789 3187 \nQ 950 3309 1192 3376 \nQ 1434 3444 1778 3444 \nQ 2416 3444 2737 3151 \nQ 3059 2859 3059 2306 \nL 3059 850 \nQ 3059 600 3125 473 \nQ 3191 347 3375 347 \nQ 3422 347 3469 353 \nQ 3516 359 3559 369 \nL 3559 19 \nQ 3453 -6 3348 -18 \nQ 3244 -31 3125 -31 \nQ 2966 -31 2852 11 \nQ 2738 53 2666 139 \nQ 2594 225 2556 351 \nQ 2519 478 2509 647 \nL 2491 647 \nQ 2400 484 2292 353 \nQ 2184 222 2040 130 \nQ 1897 38 1714 -12 \nQ 1531 -63 1294 -63 \nz\nM 1422 359 \nQ 1691 359 1892 457 \nQ 2094 556 2226 709 \nQ 2359 863 2425 1044 \nQ 2491 1225 2491 1391 \nL 2491 1669 \nL 1875 1656 \nQ 1669 1653 1483 1626 \nQ 1297 1600 1156 1522 \nQ 1016 1444 933 1303 \nQ 850 1163 850 934 \nQ 850 659 998 509 \nQ 1147 359 1422 359 \nz\nM 2108 3809 \nL 2108 4384 \nL 2617 4384 \nL 2617 3809 \nL 2108 3809 \nz\nM 877 3809 \nL 877 4384 \nL 1392 4384 \nL 1392 3809 \nL 877 3809 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-72\" d=\"M 444 0 \nL 444 2594 \nQ 444 2700 442 2811 \nQ 441 2922 437 3025 \nQ 434 3128 431 3218 \nQ 428 3309 425 3381 \nL 956 3381 \nQ 959 3309 964 3217 \nQ 969 3125 973 3028 \nQ 978 2931 979 2842 \nQ 981 2753 981 2691 \nL 994 2691 \nQ 1053 2884 1120 3026 \nQ 1188 3169 1278 3261 \nQ 1369 3353 1494 3398 \nQ 1619 3444 1797 3444 \nQ 1866 3444 1928 3433 \nQ 1991 3422 2025 3413 \nL 2025 2897 \nQ 1969 2913 1894 2920 \nQ 1819 2928 1725 2928 \nQ 1531 2928 1395 2840 \nQ 1259 2753 1173 2598 \nQ 1088 2444 1047 2230 \nQ 1006 2016 1006 1763 \nL 1006 0 \nL 444 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-e4\"/>\n       <use xlink:href=\"#LiberationSans-72\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 129.9925 303.965761 \nL 129.9925 37.853761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_6\"/>\n     <g id=\"text_3\">\n      <!-- skyldig -->\n      <g style=\"fill: #262626\" transform=\"translate(119.947395 32.764219) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-73\" d=\"M 2969 934 \nQ 2969 697 2876 511 \nQ 2784 325 2609 198 \nQ 2434 72 2179 4 \nQ 1925 -63 1597 -63 \nQ 1303 -63 1067 -17 \nQ 831 28 653 128 \nQ 475 228 354 392 \nQ 234 556 178 794 \nL 675 891 \nQ 747 619 972 492 \nQ 1197 366 1597 366 \nQ 1778 366 1929 391 \nQ 2081 416 2190 477 \nQ 2300 538 2361 639 \nQ 2422 741 2422 891 \nQ 2422 1044 2350 1142 \nQ 2278 1241 2150 1306 \nQ 2022 1372 1839 1420 \nQ 1656 1469 1438 1528 \nQ 1234 1581 1034 1647 \nQ 834 1713 673 1820 \nQ 513 1928 413 2087 \nQ 313 2247 313 2488 \nQ 313 2950 642 3192 \nQ 972 3434 1603 3434 \nQ 2163 3434 2492 3237 \nQ 2822 3041 2909 2606 \nL 2403 2544 \nQ 2375 2675 2300 2764 \nQ 2225 2853 2119 2908 \nQ 2013 2963 1880 2986 \nQ 1747 3009 1603 3009 \nQ 1222 3009 1040 2893 \nQ 859 2778 859 2544 \nQ 859 2406 926 2317 \nQ 994 2228 1114 2167 \nQ 1234 2106 1403 2061 \nQ 1572 2016 1775 1966 \nQ 1909 1931 2050 1892 \nQ 2191 1853 2323 1798 \nQ 2456 1744 2573 1670 \nQ 2691 1597 2778 1494 \nQ 2866 1391 2917 1253 \nQ 2969 1116 2969 934 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6b\" d=\"M 2550 0 \nL 1406 1544 \nL 994 1203 \nL 994 0 \nL 431 0 \nL 431 4638 \nL 994 4638 \nL 994 1741 \nL 2478 3381 \nL 3138 3381 \nL 1766 1928 \nL 3209 0 \nL 2550 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-79\" d=\"M 1888 0 \nQ 1769 -306 1645 -551 \nQ 1522 -797 1369 -970 \nQ 1216 -1144 1028 -1236 \nQ 841 -1328 597 -1328 \nQ 491 -1328 400 -1322 \nQ 309 -1316 209 -1294 \nL 209 -872 \nQ 269 -881 344 -886 \nQ 419 -891 472 -891 \nQ 719 -891 931 -706 \nQ 1144 -522 1303 -119 \nL 1356 16 \nL 16 3381 \nL 616 3381 \nL 1328 1513 \nQ 1359 1428 1407 1287 \nQ 1456 1147 1504 1006 \nQ 1553 866 1590 753 \nQ 1628 641 1634 613 \nQ 1644 647 1680 748 \nQ 1716 850 1761 975 \nQ 1806 1100 1853 1228 \nQ 1900 1356 1931 1450 \nL 2594 3381 \nL 3188 3381 \nL 1888 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6c\" d=\"M 431 0 \nL 431 4638 \nL 994 4638 \nL 994 0 \nL 431 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-64\" d=\"M 2566 544 \nQ 2409 219 2151 78 \nQ 1894 -63 1513 -63 \nQ 872 -63 570 368 \nQ 269 800 269 1675 \nQ 269 3444 1513 3444 \nQ 1897 3444 2153 3303 \nQ 2409 3163 2566 2856 \nL 2572 2856 \nQ 2572 2888 2570 2955 \nQ 2569 3022 2567 3095 \nQ 2566 3169 2566 3234 \nQ 2566 3300 2566 3328 \nL 2566 4638 \nL 3128 4638 \nL 3128 697 \nQ 3128 575 3129 462 \nQ 3131 350 3134 256 \nQ 3138 163 3141 95 \nQ 3144 28 3147 0 \nL 2609 0 \nQ 2603 31 2598 89 \nQ 2594 147 2589 222 \nQ 2584 297 2581 380 \nQ 2578 463 2578 544 \nL 2566 544 \nz\nM 859 1694 \nQ 859 1344 903 1094 \nQ 947 844 1044 683 \nQ 1141 522 1291 447 \nQ 1441 372 1656 372 \nQ 1878 372 2048 444 \nQ 2219 516 2333 677 \nQ 2447 838 2506 1097 \nQ 2566 1356 2566 1731 \nQ 2566 2091 2506 2339 \nQ 2447 2588 2331 2741 \nQ 2216 2894 2048 2961 \nQ 1881 3028 1663 3028 \nQ 1456 3028 1306 2956 \nQ 1156 2884 1056 2725 \nQ 956 2566 907 2311 \nQ 859 2056 859 1694 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-69\" d=\"M 428 4100 \nL 428 4638 \nL 991 4638 \nL 991 4100 \nL 428 4100 \nz\nM 428 0 \nL 428 3381 \nL 991 3381 \nL 991 0 \nL 428 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-67\" d=\"M 1713 -1328 \nQ 1422 -1328 1197 -1268 \nQ 972 -1209 812 -1098 \nQ 653 -988 553 -834 \nQ 453 -681 409 -494 \nL 975 -413 \nQ 1031 -647 1223 -773 \nQ 1416 -900 1728 -900 \nQ 1919 -900 2075 -847 \nQ 2231 -794 2340 -676 \nQ 2450 -559 2509 -371 \nQ 2569 -184 2569 84 \nL 2569 628 \nL 2563 628 \nQ 2500 500 2408 383 \nQ 2316 266 2183 175 \nQ 2050 84 1875 29 \nQ 1700 -25 1475 -25 \nQ 1153 -25 923 83 \nQ 694 191 548 406 \nQ 403 622 336 942 \nQ 269 1263 269 1684 \nQ 269 2091 336 2416 \nQ 403 2741 554 2967 \nQ 706 3194 948 3314 \nQ 1191 3434 1538 3434 \nQ 1897 3434 2161 3270 \nQ 2425 3106 2569 2803 \nL 2575 2803 \nQ 2575 2881 2579 2978 \nQ 2584 3075 2589 3161 \nQ 2594 3247 2600 3309 \nQ 2606 3372 2613 3381 \nL 3147 3381 \nQ 3144 3353 3141 3286 \nQ 3138 3219 3134 3125 \nQ 3131 3031 3129 2917 \nQ 3128 2803 3128 2681 \nL 3128 97 \nQ 3128 -613 2779 -970 \nQ 2431 -1328 1713 -1328 \nz\nM 2569 1691 \nQ 2569 2044 2492 2295 \nQ 2416 2547 2291 2706 \nQ 2166 2866 2005 2941 \nQ 1844 3016 1675 3016 \nQ 1459 3016 1304 2941 \nQ 1150 2866 1048 2705 \nQ 947 2544 898 2292 \nQ 850 2041 850 1691 \nQ 850 1325 898 1076 \nQ 947 828 1047 675 \nQ 1147 522 1300 456 \nQ 1453 391 1666 391 \nQ 1834 391 1995 462 \nQ 2156 534 2284 690 \nQ 2413 847 2491 1094 \nQ 2569 1341 2569 1691 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-73\"/>\n       <use xlink:href=\"#LiberationSans-6b\" x=\"50\"/>\n       <use xlink:href=\"#LiberationSans-79\" x=\"100\"/>\n       <use xlink:href=\"#LiberationSans-6c\" x=\"150\"/>\n       <use xlink:href=\"#LiberationSans-64\" x=\"172.216797\"/>\n       <use xlink:href=\"#LiberationSans-69\" x=\"227.832031\"/>\n       <use xlink:href=\"#LiberationSans-67\" x=\"250.048828\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 165.7045 303.965761 \nL 165.7045 37.853761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_8\"/>\n     <g id=\"text_4\">\n      <!-- mig -->\n      <g style=\"fill: #262626\" transform=\"translate(161.195263 32.764219) rotate(-40) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-6d\"/>\n       <use xlink:href=\"#LiberationSans-69\" x=\"83.300781\"/>\n       <use xlink:href=\"#LiberationSans-67\" x=\"105.517578\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 201.4165 303.965761 \nL 201.4165 37.853761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_10\"/>\n     <g id=\"text_5\">\n      <!-- en -->\n      <g style=\"fill: #262626\" transform=\"translate(198.818783 32.764219) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-65\" d=\"M 863 1572 \nQ 863 1306 917 1082 \nQ 972 859 1086 698 \nQ 1200 538 1378 448 \nQ 1556 359 1806 359 \nQ 2172 359 2392 506 \nQ 2613 653 2691 878 \nL 3184 738 \nQ 3131 597 3036 455 \nQ 2941 313 2781 198 \nQ 2622 84 2383 10 \nQ 2144 -63 1806 -63 \nQ 1056 -63 664 384 \nQ 272 831 272 1713 \nQ 272 2188 390 2517 \nQ 509 2847 715 3053 \nQ 922 3259 1197 3351 \nQ 1472 3444 1784 3444 \nQ 2209 3444 2495 3306 \nQ 2781 3169 2954 2926 \nQ 3128 2684 3201 2356 \nQ 3275 2028 3275 1647 \nL 3275 1572 \nL 863 1572 \nz\nM 2694 2003 \nQ 2647 2538 2422 2783 \nQ 2197 3028 1775 3028 \nQ 1634 3028 1479 2983 \nQ 1325 2938 1194 2822 \nQ 1063 2706 972 2507 \nQ 881 2309 869 2003 \nL 2694 2003 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6e\" d=\"M 2578 0 \nL 2578 2144 \nQ 2578 2391 2542 2556 \nQ 2506 2722 2425 2823 \nQ 2344 2925 2211 2967 \nQ 2078 3009 1881 3009 \nQ 1681 3009 1520 2939 \nQ 1359 2869 1245 2736 \nQ 1131 2603 1068 2408 \nQ 1006 2213 1006 1959 \nL 1006 0 \nL 444 0 \nL 444 2659 \nQ 444 2766 442 2883 \nQ 441 3000 437 3104 \nQ 434 3209 431 3284 \nQ 428 3359 425 3381 \nL 956 3381 \nQ 959 3366 962 3297 \nQ 966 3228 970 3139 \nQ 975 3050 978 2958 \nQ 981 2866 981 2803 \nL 991 2803 \nQ 1072 2950 1169 3069 \nQ 1266 3188 1394 3270 \nQ 1522 3353 1687 3398 \nQ 1853 3444 2072 3444 \nQ 2353 3444 2556 3375 \nQ 2759 3306 2890 3162 \nQ 3022 3019 3083 2792 \nQ 3144 2566 3144 2253 \nL 3144 0 \nL 2578 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-65\"/>\n       <use xlink:href=\"#LiberationSans-6e\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path d=\"M 237.1285 303.965761 \nL 237.1285 37.853761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_12\"/>\n     <g id=\"text_6\">\n      <!-- massa -->\n      <g style=\"fill: #262626\" transform=\"translate(227.510106 32.764219) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-61\" d=\"M 1294 -63 \nQ 784 -63 528 206 \nQ 272 475 272 944 \nQ 272 1278 398 1492 \nQ 525 1706 729 1828 \nQ 934 1950 1196 1997 \nQ 1459 2044 1731 2050 \nL 2491 2063 \nL 2491 2247 \nQ 2491 2456 2447 2603 \nQ 2403 2750 2312 2840 \nQ 2222 2931 2086 2973 \nQ 1950 3016 1766 3016 \nQ 1603 3016 1472 2992 \nQ 1341 2969 1244 2908 \nQ 1147 2847 1087 2742 \nQ 1028 2638 1009 2478 \nL 422 2531 \nQ 453 2731 540 2898 \nQ 628 3066 789 3187 \nQ 950 3309 1192 3376 \nQ 1434 3444 1778 3444 \nQ 2416 3444 2737 3151 \nQ 3059 2859 3059 2306 \nL 3059 850 \nQ 3059 600 3125 473 \nQ 3191 347 3375 347 \nQ 3422 347 3469 353 \nQ 3516 359 3559 369 \nL 3559 19 \nQ 3453 -6 3348 -18 \nQ 3244 -31 3125 -31 \nQ 2966 -31 2852 11 \nQ 2738 53 2666 139 \nQ 2594 225 2556 351 \nQ 2519 478 2509 647 \nL 2491 647 \nQ 2400 484 2292 353 \nQ 2184 222 2040 130 \nQ 1897 38 1714 -12 \nQ 1531 -63 1294 -63 \nz\nM 1422 359 \nQ 1691 359 1892 457 \nQ 2094 556 2226 709 \nQ 2359 863 2425 1044 \nQ 2491 1225 2491 1391 \nL 2491 1669 \nL 1875 1656 \nQ 1669 1653 1483 1626 \nQ 1297 1600 1156 1522 \nQ 1016 1444 933 1303 \nQ 850 1163 850 934 \nQ 850 659 998 509 \nQ 1147 359 1422 359 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-6d\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"83.300781\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"138.916016\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"188.916016\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"238.916016\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path d=\"M 272.8405 303.965761 \nL 272.8405 37.853761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_14\"/>\n     <g id=\"text_7\">\n      <!-- pengar -->\n      <g style=\"fill: #262626\" transform=\"translate(262.577551 32.764219) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-70\" d=\"M 3291 1706 \nQ 3291 1322 3228 997 \nQ 3166 672 3020 437 \nQ 2875 203 2636 70 \nQ 2397 -63 2047 -63 \nQ 1684 -63 1412 75 \nQ 1141 213 997 525 \nL 981 525 \nQ 984 519 986 469 \nQ 988 419 989 344 \nQ 991 269 992 176 \nQ 994 84 994 -6 \nL 994 -1328 \nL 431 -1328 \nL 431 2691 \nQ 431 2813 429 2925 \nQ 428 3038 425 3130 \nQ 422 3222 419 3287 \nQ 416 3353 413 3381 \nL 956 3381 \nQ 959 3372 964 3315 \nQ 969 3259 973 3179 \nQ 978 3100 983 3009 \nQ 988 2919 988 2838 \nL 1000 2838 \nQ 1078 3000 1178 3114 \nQ 1278 3228 1406 3301 \nQ 1534 3375 1692 3408 \nQ 1850 3441 2047 3441 \nQ 2397 3441 2636 3316 \nQ 2875 3191 3020 2964 \nQ 3166 2738 3228 2417 \nQ 3291 2097 3291 1706 \nz\nM 2700 1694 \nQ 2700 2006 2662 2250 \nQ 2625 2494 2533 2662 \nQ 2441 2831 2287 2918 \nQ 2134 3006 1903 3006 \nQ 1716 3006 1550 2953 \nQ 1384 2900 1261 2750 \nQ 1138 2600 1066 2336 \nQ 994 2072 994 1650 \nQ 994 1291 1053 1042 \nQ 1113 794 1227 641 \nQ 1341 488 1509 420 \nQ 1678 353 1897 353 \nQ 2131 353 2286 443 \nQ 2441 534 2533 706 \nQ 2625 878 2662 1126 \nQ 2700 1375 2700 1694 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-70\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-6e\" x=\"111.230469\"/>\n       <use xlink:href=\"#LiberationSans-67\" x=\"166.845703\"/>\n       <use xlink:href=\"#LiberationSans-61\" x=\"222.460938\"/>\n       <use xlink:href=\"#LiberationSans-72\" x=\"278.076172\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path d=\"M 308.5525 303.965761 \nL 308.5525 37.853761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_16\"/>\n     <g id=\"text_8\">\n      <!-- . -->\n      <g style=\"fill: #262626\" transform=\"translate(309.150625 32.764219) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-2e\" d=\"M 584 0 \nL 584 684 \nL 1194 684 \nL 1194 0 \nL 584 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-2e\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_17\">\n      <path d=\"M 40.7125 54.485761 \nL 326.4085 54.485761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_18\"/>\n     <g id=\"text_9\">\n      <!-- tom -->\n      <g style=\"fill: #262626\" transform=\"translate(20.54375 58.109199) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-74\"/>\n       <use xlink:href=\"#LiberationSans-6f\" x=\"27.783203\"/>\n       <use xlink:href=\"#LiberationSans-6d\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_19\">\n      <path d=\"M 40.7125 87.749761 \nL 326.4085 87.749761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_20\"/>\n     <g id=\"text_10\">\n      <!-- owes -->\n      <g style=\"fill: #262626\" transform=\"translate(13.86875 91.373199) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-77\" d=\"M 3669 0 \nL 3016 0 \nL 2475 2181 \nQ 2450 2266 2423 2391 \nQ 2397 2516 2372 2634 \nQ 2341 2772 2313 2919 \nQ 2284 2778 2253 2641 \nQ 2228 2522 2198 2394 \nQ 2169 2266 2144 2169 \nL 1588 0 \nL 938 0 \nL -9 3381 \nL 547 3381 \nL 1119 1084 \nQ 1141 1013 1162 908 \nQ 1184 803 1206 703 \nQ 1228 588 1253 466 \nQ 1278 584 1306 697 \nQ 1331 794 1356 894 \nQ 1381 994 1400 1059 \nL 2013 3381 \nL 2616 3381 \nL 3206 1059 \nQ 3228 978 3253 875 \nQ 3278 772 3300 681 \nQ 3325 575 3350 466 \nQ 3375 584 3400 697 \nQ 3422 794 3445 898 \nQ 3469 1003 3491 1084 \nL 4088 3381 \nL 4638 3381 \nL 3669 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-6f\"/>\n       <use xlink:href=\"#LiberationSans-77\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"127.832031\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"183.447266\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_21\">\n      <path d=\"M 40.7125 121.013761 \nL 326.4085 121.013761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_22\"/>\n     <g id=\"text_11\">\n      <!-- me -->\n      <g style=\"fill: #262626\" transform=\"translate(23.321875 124.637199) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-6d\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"83.300781\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_23\">\n      <path d=\"M 40.7125 154.277761 \nL 326.4085 154.277761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_24\"/>\n     <g id=\"text_12\">\n      <!-- a -->\n      <g style=\"fill: #262626\" transform=\"translate(31.651563 157.901199) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-61\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_25\">\n      <path d=\"M 40.7125 187.541761 \nL 326.4085 187.541761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_26\"/>\n     <g id=\"text_13\">\n      <!-- lot -->\n      <g style=\"fill: #262626\" transform=\"translate(26.651563 191.165199) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-6c\"/>\n       <use xlink:href=\"#LiberationSans-6f\" x=\"22.216797\"/>\n       <use xlink:href=\"#LiberationSans-74\" x=\"77.832031\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_27\">\n      <path d=\"M 40.7125 220.805761 \nL 326.4085 220.805761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_28\"/>\n     <g id=\"text_14\">\n      <!-- of -->\n      <g style=\"fill: #262626\" transform=\"translate(28.873438 224.429199) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-66\" d=\"M 1128 2972 \nL 1128 0 \nL 566 0 \nL 566 2972 \nL 91 2972 \nL 91 3381 \nL 566 3381 \nL 566 3763 \nQ 566 3947 600 4105 \nQ 634 4263 726 4380 \nQ 819 4497 978 4564 \nQ 1138 4631 1391 4631 \nQ 1491 4631 1598 4622 \nQ 1706 4613 1788 4594 \nL 1788 4166 \nQ 1734 4175 1664 4183 \nQ 1594 4191 1538 4191 \nQ 1413 4191 1333 4156 \nQ 1253 4122 1208 4058 \nQ 1163 3994 1145 3900 \nQ 1128 3806 1128 3684 \nL 1128 3381 \nL 1788 3381 \nL 1788 2972 \nL 1128 2972 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-6f\"/>\n       <use xlink:href=\"#LiberationSans-66\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_29\">\n      <path d=\"M 40.7125 254.069761 \nL 326.4085 254.069761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_30\"/>\n     <g id=\"text_15\">\n      <!-- money -->\n      <g style=\"fill: #262626\" transform=\"translate(7.2 257.693199) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-6d\"/>\n       <use xlink:href=\"#LiberationSans-6f\" x=\"83.300781\"/>\n       <use xlink:href=\"#LiberationSans-6e\" x=\"138.916016\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"194.53125\"/>\n       <use xlink:href=\"#LiberationSans-79\" x=\"250.146484\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_31\">\n      <path d=\"M 40.7125 287.333761 \nL 326.4085 287.333761 \n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_32\"/>\n     <g id=\"text_16\">\n      <!-- . -->\n      <g style=\"fill: #262626\" transform=\"translate(34.434375 290.957199) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-2e\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"PolyCollection_1\">\n    <path d=\"M 40.7125 37.853761 \nL 40.7125 71.117761 \nL 76.4245 71.117761 \nL 76.4245 37.853761 \nL 40.7125 37.853761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #a5cde3\"/>\n    <path d=\"M 76.4245 37.853761 \nL 76.4245 71.117761 \nL 112.1365 71.117761 \nL 112.1365 37.853761 \nL 76.4245 37.853761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #084285\"/>\n    <path d=\"M 112.1365 37.853761 \nL 112.1365 71.117761 \nL 147.8485 71.117761 \nL 147.8485 37.853761 \nL 112.1365 37.853761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #084488\"/>\n    <path d=\"M 147.8485 37.853761 \nL 147.8485 71.117761 \nL 183.5605 71.117761 \nL 183.5605 37.853761 \nL 147.8485 37.853761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #083877\"/>\n    <path d=\"M 183.5605 37.853761 \nL 183.5605 71.117761 \nL 219.2725 71.117761 \nL 219.2725 37.853761 \nL 183.5605 37.853761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #083877\"/>\n    <path d=\"M 219.2725 37.853761 \nL 219.2725 71.117761 \nL 254.9845 71.117761 \nL 254.9845 37.853761 \nL 219.2725 37.853761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #083979\"/>\n    <path d=\"M 254.9845 37.853761 \nL 254.9845 71.117761 \nL 290.6965 71.117761 \nL 290.6965 37.853761 \nL 254.9845 37.853761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08468b\"/>\n    <path d=\"M 290.6965 37.853761 \nL 290.6965 71.117761 \nL 326.4085 71.117761 \nL 326.4085 37.853761 \nL 290.6965 37.853761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #083573\"/>\n    <path d=\"M 40.7125 71.117761 \nL 40.7125 104.381761 \nL 76.4245 104.381761 \nL 76.4245 71.117761 \nL 40.7125 71.117761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #083370\"/>\n    <path d=\"M 76.4245 71.117761 \nL 76.4245 104.381761 \nL 112.1365 104.381761 \nL 112.1365 71.117761 \nL 76.4245 71.117761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #084285\"/>\n    <path d=\"M 112.1365 71.117761 \nL 112.1365 104.381761 \nL 147.8485 104.381761 \nL 147.8485 71.117761 \nL 112.1365 71.117761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #d9e8f5\"/>\n    <path d=\"M 147.8485 71.117761 \nL 147.8485 104.381761 \nL 183.5605 104.381761 \nL 183.5605 71.117761 \nL 147.8485 71.117761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #084082\"/>\n    <path d=\"M 183.5605 71.117761 \nL 183.5605 104.381761 \nL 219.2725 104.381761 \nL 219.2725 71.117761 \nL 183.5605 71.117761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08316d\"/>\n    <path d=\"M 219.2725 71.117761 \nL 219.2725 104.381761 \nL 254.9845 104.381761 \nL 254.9845 71.117761 \nL 219.2725 71.117761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 254.9845 71.117761 \nL 254.9845 104.381761 \nL 290.6965 104.381761 \nL 290.6965 71.117761 \nL 254.9845 71.117761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 290.6965 71.117761 \nL 290.6965 104.381761 \nL 326.4085 104.381761 \nL 326.4085 71.117761 \nL 290.6965 71.117761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 40.7125 104.381761 \nL 40.7125 137.645761 \nL 76.4245 137.645761 \nL 76.4245 104.381761 \nL 40.7125 104.381761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 76.4245 104.381761 \nL 76.4245 137.645761 \nL 112.1365 137.645761 \nL 112.1365 104.381761 \nL 76.4245 104.381761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 112.1365 104.381761 \nL 112.1365 137.645761 \nL 147.8485 137.645761 \nL 147.8485 104.381761 \nL 112.1365 104.381761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #d1e2f3\"/>\n    <path d=\"M 147.8485 104.381761 \nL 147.8485 137.645761 \nL 183.5605 137.645761 \nL 183.5605 104.381761 \nL 147.8485 104.381761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #084990\"/>\n    <path d=\"M 183.5605 104.381761 \nL 183.5605 137.645761 \nL 219.2725 137.645761 \nL 219.2725 104.381761 \nL 183.5605 104.381761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #084387\"/>\n    <path d=\"M 219.2725 104.381761 \nL 219.2725 137.645761 \nL 254.9845 137.645761 \nL 254.9845 104.381761 \nL 219.2725 104.381761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08316d\"/>\n    <path d=\"M 254.9845 104.381761 \nL 254.9845 137.645761 \nL 290.6965 137.645761 \nL 290.6965 104.381761 \nL 254.9845 104.381761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #083370\"/>\n    <path d=\"M 290.6965 104.381761 \nL 290.6965 137.645761 \nL 326.4085 137.645761 \nL 326.4085 104.381761 \nL 290.6965 104.381761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 40.7125 137.645761 \nL 40.7125 170.909761 \nL 76.4245 170.909761 \nL 76.4245 137.645761 \nL 40.7125 137.645761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 76.4245 137.645761 \nL 76.4245 170.909761 \nL 112.1365 170.909761 \nL 112.1365 137.645761 \nL 76.4245 137.645761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 112.1365 137.645761 \nL 112.1365 170.909761 \nL 147.8485 170.909761 \nL 147.8485 137.645761 \nL 112.1365 137.645761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #083a7a\"/>\n    <path d=\"M 147.8485 137.645761 \nL 147.8485 170.909761 \nL 183.5605 170.909761 \nL 183.5605 137.645761 \nL 147.8485 137.645761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08326e\"/>\n    <path d=\"M 183.5605 137.645761 \nL 183.5605 170.909761 \nL 219.2725 170.909761 \nL 219.2725 137.645761 \nL 183.5605 137.645761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #c9ddf0\"/>\n    <path d=\"M 219.2725 137.645761 \nL 219.2725 170.909761 \nL 254.9845 170.909761 \nL 254.9845 137.645761 \nL 219.2725 137.645761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #09529d\"/>\n    <path d=\"M 254.9845 137.645761 \nL 254.9845 170.909761 \nL 290.6965 170.909761 \nL 290.6965 137.645761 \nL 254.9845 137.645761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #083b7c\"/>\n    <path d=\"M 290.6965 137.645761 \nL 290.6965 170.909761 \nL 326.4085 170.909761 \nL 326.4085 137.645761 \nL 290.6965 137.645761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08316d\"/>\n    <path d=\"M 40.7125 170.909761 \nL 40.7125 204.173761 \nL 76.4245 204.173761 \nL 76.4245 170.909761 \nL 40.7125 170.909761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 76.4245 170.909761 \nL 76.4245 204.173761 \nL 112.1365 204.173761 \nL 112.1365 170.909761 \nL 76.4245 170.909761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 112.1365 170.909761 \nL 112.1365 204.173761 \nL 147.8485 204.173761 \nL 147.8485 170.909761 \nL 112.1365 170.909761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 147.8485 170.909761 \nL 147.8485 204.173761 \nL 183.5605 204.173761 \nL 183.5605 170.909761 \nL 147.8485 170.909761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 183.5605 170.909761 \nL 183.5605 204.173761 \nL 219.2725 204.173761 \nL 219.2725 170.909761 \nL 183.5605 170.909761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #125da6\"/>\n    <path d=\"M 219.2725 170.909761 \nL 219.2725 204.173761 \nL 254.9845 204.173761 \nL 254.9845 170.909761 \nL 219.2725 170.909761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #519ccc\"/>\n    <path d=\"M 254.9845 170.909761 \nL 254.9845 204.173761 \nL 290.6965 204.173761 \nL 290.6965 170.909761 \nL 254.9845 170.909761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #4493c7\"/>\n    <path d=\"M 290.6965 170.909761 \nL 290.6965 204.173761 \nL 326.4085 204.173761 \nL 326.4085 170.909761 \nL 290.6965 170.909761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #083471\"/>\n    <path d=\"M 40.7125 204.173761 \nL 40.7125 237.437761 \nL 76.4245 237.437761 \nL 76.4245 204.173761 \nL 40.7125 204.173761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 76.4245 204.173761 \nL 76.4245 237.437761 \nL 112.1365 237.437761 \nL 112.1365 204.173761 \nL 76.4245 204.173761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 112.1365 204.173761 \nL 112.1365 237.437761 \nL 147.8485 237.437761 \nL 147.8485 204.173761 \nL 112.1365 204.173761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 147.8485 204.173761 \nL 147.8485 237.437761 \nL 183.5605 237.437761 \nL 183.5605 204.173761 \nL 147.8485 204.173761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 183.5605 204.173761 \nL 183.5605 237.437761 \nL 219.2725 237.437761 \nL 219.2725 204.173761 \nL 183.5605 204.173761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08316d\"/>\n    <path d=\"M 219.2725 204.173761 \nL 219.2725 237.437761 \nL 254.9845 237.437761 \nL 254.9845 204.173761 \nL 219.2725 204.173761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #083b7c\"/>\n    <path d=\"M 254.9845 204.173761 \nL 254.9845 237.437761 \nL 290.6965 237.437761 \nL 290.6965 204.173761 \nL 254.9845 204.173761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #edf4fc\"/>\n    <path d=\"M 290.6965 204.173761 \nL 290.6965 237.437761 \nL 326.4085 237.437761 \nL 326.4085 204.173761 \nL 290.6965 204.173761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 40.7125 237.437761 \nL 40.7125 270.701761 \nL 76.4245 270.701761 \nL 76.4245 237.437761 \nL 40.7125 237.437761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 76.4245 237.437761 \nL 76.4245 270.701761 \nL 112.1365 270.701761 \nL 112.1365 237.437761 \nL 76.4245 237.437761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 112.1365 237.437761 \nL 112.1365 270.701761 \nL 147.8485 270.701761 \nL 147.8485 237.437761 \nL 112.1365 237.437761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 147.8485 237.437761 \nL 147.8485 270.701761 \nL 183.5605 270.701761 \nL 183.5605 237.437761 \nL 147.8485 237.437761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 183.5605 237.437761 \nL 183.5605 270.701761 \nL 219.2725 270.701761 \nL 219.2725 237.437761 \nL 183.5605 237.437761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08316d\"/>\n    <path d=\"M 219.2725 237.437761 \nL 219.2725 270.701761 \nL 254.9845 270.701761 \nL 254.9845 237.437761 \nL 219.2725 237.437761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #2171b5\"/>\n    <path d=\"M 254.9845 237.437761 \nL 254.9845 270.701761 \nL 290.6965 270.701761 \nL 290.6965 237.437761 \nL 254.9845 237.437761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #bed8ec\"/>\n    <path d=\"M 290.6965 237.437761 \nL 290.6965 270.701761 \nL 326.4085 270.701761 \nL 326.4085 237.437761 \nL 290.6965 237.437761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #083471\"/>\n    <path d=\"M 40.7125 270.701761 \nL 40.7125 303.965761 \nL 76.4245 303.965761 \nL 76.4245 270.701761 \nL 40.7125 270.701761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 76.4245 270.701761 \nL 76.4245 303.965761 \nL 112.1365 303.965761 \nL 112.1365 270.701761 \nL 76.4245 270.701761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 112.1365 270.701761 \nL 112.1365 303.965761 \nL 147.8485 303.965761 \nL 147.8485 270.701761 \nL 112.1365 270.701761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 147.8485 270.701761 \nL 147.8485 303.965761 \nL 183.5605 303.965761 \nL 183.5605 270.701761 \nL 147.8485 270.701761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 183.5605 270.701761 \nL 183.5605 303.965761 \nL 219.2725 303.965761 \nL 219.2725 270.701761 \nL 183.5605 270.701761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08306b\"/>\n    <path d=\"M 219.2725 270.701761 \nL 219.2725 303.965761 \nL 254.9845 303.965761 \nL 254.9845 270.701761 \nL 219.2725 270.701761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #105ba4\"/>\n    <path d=\"M 254.9845 270.701761 \nL 254.9845 303.965761 \nL 290.6965 303.965761 \nL 290.6965 270.701761 \nL 254.9845 270.701761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #c4daee\"/>\n    <path d=\"M 290.6965 270.701761 \nL 290.6965 303.965761 \nL 326.4085 303.965761 \nL 326.4085 270.701761 \nL 290.6965 270.701761 \nz\n\" clip-path=\"url(#pafc0725f5b)\" style=\"fill: #08478d\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.7125 303.965761 \nL 40.7125 37.853761 \n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 326.4085 303.965761 \nL 326.4085 37.853761 \n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.7125 303.965761 \nL 326.4085 303.965761 \n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.7125 37.853761 \nL 326.4085 37.853761 \n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 344.2645 303.965761 \nL 357.5701 303.965761 \nL 357.5701 37.853761 \nL 344.2645 37.853761 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_9\">\n     <g id=\"line2d_33\"/>\n     <g id=\"text_17\">\n      <!-- 0.0 -->\n      <g style=\"fill: #262626\" transform=\"translate(361.0701 307.589199) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-30\" d=\"M 3309 2203 \nQ 3309 1569 3189 1136 \nQ 3069 703 2861 436 \nQ 2653 169 2372 53 \nQ 2091 -63 1772 -63 \nQ 1450 -63 1172 53 \nQ 894 169 689 434 \nQ 484 700 367 1133 \nQ 250 1566 250 2203 \nQ 250 2869 367 3305 \nQ 484 3741 690 4000 \nQ 897 4259 1178 4364 \nQ 1459 4469 1791 4469 \nQ 2106 4469 2382 4364 \nQ 2659 4259 2865 4000 \nQ 3072 3741 3190 3305 \nQ 3309 2869 3309 2203 \nz\nM 2738 2203 \nQ 2738 2728 2675 3076 \nQ 2613 3425 2491 3633 \nQ 2369 3841 2192 3927 \nQ 2016 4013 1791 4013 \nQ 1553 4013 1372 3925 \nQ 1191 3838 1067 3630 \nQ 944 3422 881 3073 \nQ 819 2725 819 2203 \nQ 819 1697 883 1350 \nQ 947 1003 1070 792 \nQ 1194 581 1372 489 \nQ 1550 397 1778 397 \nQ 2000 397 2178 489 \nQ 2356 581 2479 792 \nQ 2603 1003 2670 1350 \nQ 2738 1697 2738 2203 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-30\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_34\"/>\n     <g id=\"text_18\">\n      <!-- 0.2 -->\n      <g style=\"fill: #262626\" transform=\"translate(361.0701 254.366799) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-32\" d=\"M 322 0 \nL 322 397 \nQ 481 763 711 1042 \nQ 941 1322 1194 1548 \nQ 1447 1775 1695 1969 \nQ 1944 2163 2144 2356 \nQ 2344 2550 2467 2762 \nQ 2591 2975 2591 3244 \nQ 2591 3431 2534 3573 \nQ 2478 3716 2372 3812 \nQ 2266 3909 2117 3957 \nQ 1969 4006 1788 4006 \nQ 1619 4006 1470 3959 \nQ 1322 3913 1206 3819 \nQ 1091 3725 1017 3586 \nQ 944 3447 922 3263 \nL 347 3316 \nQ 375 3553 478 3762 \nQ 581 3972 762 4130 \nQ 944 4288 1198 4378 \nQ 1453 4469 1788 4469 \nQ 2116 4469 2372 4391 \nQ 2628 4313 2804 4159 \nQ 2981 4006 3075 3781 \nQ 3169 3556 3169 3263 \nQ 3169 3041 3089 2841 \nQ 3009 2641 2876 2459 \nQ 2744 2278 2569 2109 \nQ 2394 1941 2203 1780 \nQ 2013 1619 1819 1461 \nQ 1625 1303 1454 1143 \nQ 1284 984 1150 820 \nQ 1016 656 941 478 \nL 3238 478 \nL 3238 0 \nL 322 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-32\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_35\"/>\n     <g id=\"text_19\">\n      <!-- 0.4 -->\n      <g style=\"fill: #262626\" transform=\"translate(361.0701 201.144399) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-34\" d=\"M 2753 997 \nL 2753 0 \nL 2222 0 \nL 2222 997 \nL 147 997 \nL 147 1434 \nL 2163 4403 \nL 2753 4403 \nL 2753 1441 \nL 3372 1441 \nL 3372 997 \nL 2753 997 \nz\nM 2222 3769 \nQ 2216 3753 2191 3708 \nQ 2166 3663 2134 3606 \nQ 2103 3550 2070 3492 \nQ 2038 3434 2013 3397 \nL 884 1734 \nQ 869 1709 839 1668 \nQ 809 1628 778 1586 \nQ 747 1544 715 1503 \nQ 684 1463 666 1441 \nL 2222 1441 \nL 2222 3769 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-34\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_36\"/>\n     <g id=\"text_20\">\n      <!-- 0.6 -->\n      <g style=\"fill: #262626\" transform=\"translate(361.0701 147.921999) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-36\" d=\"M 3278 1441 \nQ 3278 1109 3186 832 \nQ 3094 556 2914 357 \nQ 2734 159 2468 48 \nQ 2203 -63 1856 -63 \nQ 1472 -63 1184 84 \nQ 897 231 706 507 \nQ 516 784 420 1186 \nQ 325 1588 325 2100 \nQ 325 2688 433 3131 \nQ 541 3575 744 3872 \nQ 947 4169 1239 4319 \nQ 1531 4469 1900 4469 \nQ 2125 4469 2322 4422 \nQ 2519 4375 2680 4270 \nQ 2841 4166 2962 3994 \nQ 3084 3822 3156 3572 \nL 2619 3475 \nQ 2531 3759 2339 3886 \nQ 2147 4013 1894 4013 \nQ 1663 4013 1475 3903 \nQ 1288 3794 1156 3576 \nQ 1025 3359 954 3031 \nQ 884 2703 884 2266 \nQ 1038 2550 1316 2698 \nQ 1594 2847 1953 2847 \nQ 2253 2847 2497 2750 \nQ 2741 2653 2914 2470 \nQ 3088 2288 3183 2027 \nQ 3278 1766 3278 1441 \nz\nM 2706 1416 \nQ 2706 1644 2650 1828 \nQ 2594 2013 2481 2142 \nQ 2369 2272 2203 2342 \nQ 2038 2413 1819 2413 \nQ 1666 2413 1509 2367 \nQ 1353 2322 1226 2220 \nQ 1100 2119 1020 1953 \nQ 941 1788 941 1550 \nQ 941 1306 1003 1095 \nQ 1066 884 1183 728 \nQ 1300 572 1465 481 \nQ 1631 391 1838 391 \nQ 2041 391 2202 461 \nQ 2363 531 2475 664 \nQ 2588 797 2647 987 \nQ 2706 1178 2706 1416 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-36\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_37\"/>\n     <g id=\"text_21\">\n      <!-- 0.8 -->\n      <g style=\"fill: #262626\" transform=\"translate(361.0701 94.699599) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-38\" d=\"M 3281 1228 \nQ 3281 947 3192 711 \nQ 3103 475 2920 303 \nQ 2738 131 2453 34 \nQ 2169 -63 1781 -63 \nQ 1394 -63 1111 34 \nQ 828 131 642 301 \nQ 456 472 367 708 \nQ 278 944 278 1222 \nQ 278 1463 351 1650 \nQ 425 1838 548 1973 \nQ 672 2109 830 2192 \nQ 988 2275 1156 2303 \nL 1156 2316 \nQ 972 2359 826 2456 \nQ 681 2553 582 2689 \nQ 484 2825 432 2990 \nQ 381 3156 381 3341 \nQ 381 3572 470 3776 \nQ 559 3981 734 4136 \nQ 909 4291 1168 4380 \nQ 1428 4469 1769 4469 \nQ 2128 4469 2392 4378 \nQ 2656 4288 2829 4133 \nQ 3003 3978 3087 3772 \nQ 3172 3566 3172 3334 \nQ 3172 3153 3120 2987 \nQ 3069 2822 2970 2686 \nQ 2872 2550 2726 2454 \nQ 2581 2359 2391 2322 \nL 2391 2309 \nQ 2581 2278 2743 2195 \nQ 2906 2113 3025 1977 \nQ 3144 1841 3212 1653 \nQ 3281 1466 3281 1228 \nz\nM 2588 3303 \nQ 2588 3469 2545 3606 \nQ 2503 3744 2406 3842 \nQ 2309 3941 2153 3995 \nQ 1997 4050 1769 4050 \nQ 1547 4050 1394 3995 \nQ 1241 3941 1142 3842 \nQ 1044 3744 1000 3606 \nQ 956 3469 956 3303 \nQ 956 3172 990 3034 \nQ 1025 2897 1115 2784 \nQ 1206 2672 1365 2600 \nQ 1525 2528 1775 2528 \nQ 2041 2528 2202 2600 \nQ 2363 2672 2448 2784 \nQ 2534 2897 2561 3034 \nQ 2588 3172 2588 3303 \nz\nM 2697 1281 \nQ 2697 1441 2653 1589 \nQ 2609 1738 2503 1852 \nQ 2397 1966 2217 2036 \nQ 2038 2106 1769 2106 \nQ 1522 2106 1348 2036 \nQ 1175 1966 1067 1850 \nQ 959 1734 909 1582 \nQ 859 1431 859 1269 \nQ 859 1066 909 898 \nQ 959 731 1068 611 \nQ 1178 491 1356 425 \nQ 1534 359 1788 359 \nQ 2044 359 2219 425 \nQ 2394 491 2500 611 \nQ 2606 731 2651 901 \nQ 2697 1072 2697 1281 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-38\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_38\"/>\n     <g id=\"text_22\">\n      <!-- 1.0 -->\n      <g style=\"fill: #262626\" transform=\"translate(361.0701 41.477199) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-31\" d=\"M 488 0 \nL 488 478 \nL 1609 478 \nL 1609 3866 \nL 616 3156 \nL 616 3688 \nL 1656 4403 \nL 2175 4403 \nL 2175 478 \nL 3247 478 \nL 3247 0 \nL 488 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-31\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-30\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAABMAAAFyCAYAAAAXj+GHAAABx0lEQVR4nO2cwW0EMQwD7cU+Uk76Sv/vSw0SBrgBSBZALEWJsn1I7s/v3+dAeM99KK7DMZ1z3nMvSVaZQ4TIRL+MrlllzslCZGoH/emgD1GZK7LKHCJEpjkcrTWrzDlSZF5yNlGyw3Gl1Kwy50D7TCxT66a4Zm2N75LVzTm8bopr1tb4LlndnMPrprhmbY0NGcZVmQt4Zb4P+HsAXTOUrDKHCJmAEJnq1kDJKnOIypzDKzMnHNsaQ1TmHPBpu4M+JgMvKGaZYjfBJ5e6uSAjU0Mcjt0BY9CDTpJpZZIR5E0NcTim7ACMip8AadOKUyNlB2S4SbKZZbJNC5KF1My70dEdQH6aOM9CYhtOjdZsiD7fzNHttCALCUdtzegzrfRWJ3bT27Ta2TS3Bknm3QHoO614AlAybdOyT6sYVc4EeGXWzQWZdtC15zP0img+BWXI9E5AyFMEKjNk0Nk7ulYm7CYH8Y8LXpns3yJaU0N82BP/h5m6OSarmwsyjislHLvR56ibGzKOq+G4QFfdHHVzQdZwXJBxXF11C6S4ico8Dcc5mVgm6KZ5o5NkYplSN1PCMWPVoV/mlRnipvceIA5H7TjVzQ2ZVeY/J9EI0K+fCH8AAAAASUVORK5CYII=\" id=\"image087320b2c0\" transform=\"scale(1 -1) translate(0 -266.4)\" x=\"344.16\" y=\"-37.44\" width=\"13.68\" height=\"266.4\"/>\n   <g id=\"LineCollection_1\"/>\n   <g id=\"patch_8\">\n    <path d=\"M 344.2645 303.965761 \nL 350.9173 303.965761 \nL 357.5701 303.965761 \nL 357.5701 37.853761 \nL 350.9173 37.853761 \nL 344.2645 37.853761 \nL 344.2645 303.965761 \nz\n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pafc0725f5b\">\n   <rect x=\"40.7125\" y=\"37.853761\" width=\"285.696\" height=\"266.112\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"379.960725pt\" height=\"320.063071pt\" viewBox=\"0 0 379.960725 320.063071\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-05-14T22:50:04.270360</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 320.063071 \nL 379.960725 320.063071 \nL 379.960725 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 38.503125 307.164634 \nL 324.199125 307.164634 \nL 324.199125 41.052634 \nL 38.503125 41.052634 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 58.909982 307.164634 \nL 58.909982 41.052634 \n\" clip-path=\"url(#pa76db48965)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_2\"/>\n     <g id=\"text_1\">\n      <!-- då -->\n      <g style=\"fill: #262626\" transform=\"translate(56.312265 35.963092) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-64\" d=\"M 2566 544 \nQ 2409 219 2151 78 \nQ 1894 -63 1513 -63 \nQ 872 -63 570 368 \nQ 269 800 269 1675 \nQ 269 3444 1513 3444 \nQ 1897 3444 2153 3303 \nQ 2409 3163 2566 2856 \nL 2572 2856 \nQ 2572 2888 2570 2955 \nQ 2569 3022 2567 3095 \nQ 2566 3169 2566 3234 \nQ 2566 3300 2566 3328 \nL 2566 4638 \nL 3128 4638 \nL 3128 697 \nQ 3128 575 3129 462 \nQ 3131 350 3134 256 \nQ 3138 163 3141 95 \nQ 3144 28 3147 0 \nL 2609 0 \nQ 2603 31 2598 89 \nQ 2594 147 2589 222 \nQ 2584 297 2581 380 \nQ 2578 463 2578 544 \nL 2566 544 \nz\nM 859 1694 \nQ 859 1344 903 1094 \nQ 947 844 1044 683 \nQ 1141 522 1291 447 \nQ 1441 372 1656 372 \nQ 1878 372 2048 444 \nQ 2219 516 2333 677 \nQ 2447 838 2506 1097 \nQ 2566 1356 2566 1731 \nQ 2566 2091 2506 2339 \nQ 2447 2588 2331 2741 \nQ 2216 2894 2048 2961 \nQ 1881 3028 1663 3028 \nQ 1456 3028 1306 2956 \nQ 1156 2884 1056 2725 \nQ 956 2566 907 2311 \nQ 859 2056 859 1694 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-e5\" d=\"M 1294 -63 \nQ 784 -63 528 206 \nQ 272 475 272 944 \nQ 272 1278 398 1492 \nQ 525 1706 729 1828 \nQ 934 1950 1196 1997 \nQ 1459 2044 1731 2050 \nL 2491 2063 \nL 2491 2247 \nQ 2491 2456 2447 2603 \nQ 2403 2750 2312 2840 \nQ 2222 2931 2086 2973 \nQ 1950 3016 1766 3016 \nQ 1603 3016 1472 2992 \nQ 1341 2969 1244 2908 \nQ 1147 2847 1087 2742 \nQ 1028 2638 1009 2478 \nL 422 2531 \nQ 453 2731 540 2898 \nQ 628 3066 789 3187 \nQ 950 3309 1192 3376 \nQ 1434 3444 1778 3444 \nQ 2416 3444 2737 3151 \nQ 3059 2859 3059 2306 \nL 3059 850 \nQ 3059 600 3125 473 \nQ 3191 347 3375 347 \nQ 3422 347 3469 353 \nQ 3516 359 3559 369 \nL 3559 19 \nQ 3453 -6 3348 -18 \nQ 3244 -31 3125 -31 \nQ 2966 -31 2852 11 \nQ 2738 53 2666 139 \nQ 2594 225 2556 351 \nQ 2519 478 2509 647 \nL 2491 647 \nQ 2400 484 2292 353 \nQ 2184 222 2040 130 \nQ 1897 38 1714 -12 \nQ 1531 -63 1294 -63 \nz\nM 1422 359 \nQ 1691 359 1892 457 \nQ 2094 556 2226 709 \nQ 2359 863 2425 1044 \nQ 2491 1225 2491 1391 \nL 2491 1669 \nL 1875 1656 \nQ 1669 1653 1483 1626 \nQ 1297 1600 1156 1522 \nQ 1016 1444 933 1303 \nQ 850 1163 850 934 \nQ 850 659 998 509 \nQ 1147 359 1422 359 \nz\nM 2496 4406 \nQ 2496 4250 2436 4112 \nQ 2377 3975 2274 3872 \nQ 2171 3769 2033 3709 \nQ 1896 3650 1740 3650 \nQ 1583 3650 1445 3709 \nQ 1308 3769 1205 3872 \nQ 1102 3975 1042 4112 \nQ 983 4250 983 4406 \nQ 983 4563 1042 4700 \nQ 1102 4838 1205 4939 \nQ 1308 5041 1445 5100 \nQ 1583 5159 1740 5159 \nQ 1896 5159 2033 5100 \nQ 2171 5041 2274 4939 \nQ 2377 4838 2436 4700 \nQ 2496 4563 2496 4406 \nz\nM 2158 4406 \nQ 2158 4494 2125 4570 \nQ 2093 4647 2036 4703 \nQ 1980 4759 1903 4792 \nQ 1827 4825 1740 4825 \nQ 1649 4825 1572 4792 \nQ 1496 4759 1439 4703 \nQ 1383 4647 1350 4570 \nQ 1318 4494 1318 4406 \nQ 1318 4316 1350 4239 \nQ 1383 4163 1439 4103 \nQ 1496 4044 1572 4011 \nQ 1649 3978 1740 3978 \nQ 1827 3978 1903 4011 \nQ 1980 4044 2036 4103 \nQ 2093 4163 2125 4239 \nQ 2158 4316 2158 4406 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-64\"/>\n       <use xlink:href=\"#LiberationSans-e5\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 99.723696 307.164634 \nL 99.723696 41.052634 \n\" clip-path=\"url(#pa76db48965)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_4\"/>\n     <g id=\"text_2\">\n      <!-- kommer -->\n      <g style=\"fill: #262626\" transform=\"translate(87.554614 35.963092) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-6b\" d=\"M 2550 0 \nL 1406 1544 \nL 994 1203 \nL 994 0 \nL 431 0 \nL 431 4638 \nL 994 4638 \nL 994 1741 \nL 2478 3381 \nL 3138 3381 \nL 1766 1928 \nL 3209 0 \nL 2550 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6f\" d=\"M 3291 1694 \nQ 3291 806 2900 371 \nQ 2509 -63 1766 -63 \nQ 1413 -63 1134 43 \nQ 856 150 664 369 \nQ 472 588 370 917 \nQ 269 1247 269 1694 \nQ 269 3444 1784 3444 \nQ 2178 3444 2464 3334 \nQ 2750 3225 2933 3006 \nQ 3116 2788 3203 2459 \nQ 3291 2131 3291 1694 \nz\nM 2700 1694 \nQ 2700 2088 2639 2344 \nQ 2578 2600 2461 2753 \nQ 2344 2906 2175 2967 \nQ 2006 3028 1794 3028 \nQ 1578 3028 1404 2964 \nQ 1231 2900 1109 2745 \nQ 988 2591 923 2334 \nQ 859 2078 859 1694 \nQ 859 1300 928 1042 \nQ 997 784 1117 631 \nQ 1238 478 1402 415 \nQ 1566 353 1759 353 \nQ 1975 353 2150 414 \nQ 2325 475 2447 628 \nQ 2569 781 2634 1040 \nQ 2700 1300 2700 1694 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6d\" d=\"M 2400 0 \nL 2400 2144 \nQ 2400 2391 2369 2556 \nQ 2338 2722 2264 2823 \nQ 2191 2925 2072 2967 \nQ 1953 3009 1781 3009 \nQ 1603 3009 1459 2939 \nQ 1316 2869 1214 2736 \nQ 1113 2603 1058 2408 \nQ 1003 2213 1003 1959 \nL 1003 0 \nL 444 0 \nL 444 2659 \nQ 444 2766 442 2883 \nQ 441 3000 437 3104 \nQ 434 3209 431 3284 \nQ 428 3359 425 3381 \nL 956 3381 \nQ 959 3366 962 3297 \nQ 966 3228 970 3139 \nQ 975 3050 978 2958 \nQ 981 2866 981 2803 \nL 991 2803 \nQ 1066 2950 1153 3069 \nQ 1241 3188 1358 3270 \nQ 1475 3353 1626 3398 \nQ 1778 3444 1978 3444 \nQ 2363 3444 2586 3291 \nQ 2809 3138 2897 2803 \nL 2906 2803 \nQ 2981 2950 3075 3069 \nQ 3169 3188 3294 3270 \nQ 3419 3353 3575 3398 \nQ 3731 3444 3931 3444 \nQ 4188 3444 4373 3375 \nQ 4559 3306 4678 3162 \nQ 4797 3019 4853 2792 \nQ 4909 2566 4909 2253 \nL 4909 0 \nL 4353 0 \nL 4353 2144 \nQ 4353 2391 4322 2556 \nQ 4291 2722 4217 2823 \nQ 4144 2925 4025 2967 \nQ 3906 3009 3734 3009 \nQ 3556 3009 3412 2942 \nQ 3269 2875 3167 2744 \nQ 3066 2613 3011 2416 \nQ 2956 2219 2956 1959 \nL 2956 0 \nL 2400 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-65\" d=\"M 863 1572 \nQ 863 1306 917 1082 \nQ 972 859 1086 698 \nQ 1200 538 1378 448 \nQ 1556 359 1806 359 \nQ 2172 359 2392 506 \nQ 2613 653 2691 878 \nL 3184 738 \nQ 3131 597 3036 455 \nQ 2941 313 2781 198 \nQ 2622 84 2383 10 \nQ 2144 -63 1806 -63 \nQ 1056 -63 664 384 \nQ 272 831 272 1713 \nQ 272 2188 390 2517 \nQ 509 2847 715 3053 \nQ 922 3259 1197 3351 \nQ 1472 3444 1784 3444 \nQ 2209 3444 2495 3306 \nQ 2781 3169 2954 2926 \nQ 3128 2684 3201 2356 \nQ 3275 2028 3275 1647 \nL 3275 1572 \nL 863 1572 \nz\nM 2694 2003 \nQ 2647 2538 2422 2783 \nQ 2197 3028 1775 3028 \nQ 1634 3028 1479 2983 \nQ 1325 2938 1194 2822 \nQ 1063 2706 972 2507 \nQ 881 2309 869 2003 \nL 2694 2003 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-72\" d=\"M 444 0 \nL 444 2594 \nQ 444 2700 442 2811 \nQ 441 2922 437 3025 \nQ 434 3128 431 3218 \nQ 428 3309 425 3381 \nL 956 3381 \nQ 959 3309 964 3217 \nQ 969 3125 973 3028 \nQ 978 2931 979 2842 \nQ 981 2753 981 2691 \nL 994 2691 \nQ 1053 2884 1120 3026 \nQ 1188 3169 1278 3261 \nQ 1369 3353 1494 3398 \nQ 1619 3444 1797 3444 \nQ 1866 3444 1928 3433 \nQ 1991 3422 2025 3413 \nL 2025 2897 \nQ 1969 2913 1894 2920 \nQ 1819 2928 1725 2928 \nQ 1531 2928 1395 2840 \nQ 1259 2753 1173 2598 \nQ 1088 2444 1047 2230 \nQ 1006 2016 1006 1763 \nL 1006 0 \nL 444 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-6b\"/>\n       <use xlink:href=\"#LiberationSans-6f\" x=\"50\"/>\n       <use xlink:href=\"#LiberationSans-6d\" x=\"105.615234\"/>\n       <use xlink:href=\"#LiberationSans-6d\" x=\"188.916016\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"272.216797\"/>\n       <use xlink:href=\"#LiberationSans-72\" x=\"327.832031\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 140.537411 307.164634 \nL 140.537411 41.052634 \n\" clip-path=\"url(#pa76db48965)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_6\"/>\n     <g id=\"text_3\">\n      <!-- mormor -->\n      <g style=\"fill: #262626\" transform=\"translate(129.008095 35.963092) rotate(-40) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-6d\"/>\n       <use xlink:href=\"#LiberationSans-6f\" x=\"83.300781\"/>\n       <use xlink:href=\"#LiberationSans-72\" x=\"138.916016\"/>\n       <use xlink:href=\"#LiberationSans-6d\" x=\"172.216797\"/>\n       <use xlink:href=\"#LiberationSans-6f\" x=\"255.517578\"/>\n       <use xlink:href=\"#LiberationSans-72\" x=\"311.132812\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 181.351125 307.164634 \nL 181.351125 41.052634 \n\" clip-path=\"url(#pa76db48965)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_8\"/>\n     <g id=\"text_4\">\n      <!-- att -->\n      <g style=\"fill: #262626\" transform=\"translate(178.755204 35.963092) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-61\" d=\"M 1294 -63 \nQ 784 -63 528 206 \nQ 272 475 272 944 \nQ 272 1278 398 1492 \nQ 525 1706 729 1828 \nQ 934 1950 1196 1997 \nQ 1459 2044 1731 2050 \nL 2491 2063 \nL 2491 2247 \nQ 2491 2456 2447 2603 \nQ 2403 2750 2312 2840 \nQ 2222 2931 2086 2973 \nQ 1950 3016 1766 3016 \nQ 1603 3016 1472 2992 \nQ 1341 2969 1244 2908 \nQ 1147 2847 1087 2742 \nQ 1028 2638 1009 2478 \nL 422 2531 \nQ 453 2731 540 2898 \nQ 628 3066 789 3187 \nQ 950 3309 1192 3376 \nQ 1434 3444 1778 3444 \nQ 2416 3444 2737 3151 \nQ 3059 2859 3059 2306 \nL 3059 850 \nQ 3059 600 3125 473 \nQ 3191 347 3375 347 \nQ 3422 347 3469 353 \nQ 3516 359 3559 369 \nL 3559 19 \nQ 3453 -6 3348 -18 \nQ 3244 -31 3125 -31 \nQ 2966 -31 2852 11 \nQ 2738 53 2666 139 \nQ 2594 225 2556 351 \nQ 2519 478 2509 647 \nL 2491 647 \nQ 2400 484 2292 353 \nQ 2184 222 2040 130 \nQ 1897 38 1714 -12 \nQ 1531 -63 1294 -63 \nz\nM 1422 359 \nQ 1691 359 1892 457 \nQ 2094 556 2226 709 \nQ 2359 863 2425 1044 \nQ 2491 1225 2491 1391 \nL 2491 1669 \nL 1875 1656 \nQ 1669 1653 1483 1626 \nQ 1297 1600 1156 1522 \nQ 1016 1444 933 1303 \nQ 850 1163 850 934 \nQ 850 659 998 509 \nQ 1147 359 1422 359 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-74\" d=\"M 1731 25 \nQ 1603 -9 1470 -29 \nQ 1338 -50 1163 -50 \nQ 488 -50 488 716 \nL 488 2972 \nL 97 2972 \nL 97 3381 \nL 509 3381 \nL 675 4138 \nL 1050 4138 \nL 1050 3381 \nL 1675 3381 \nL 1675 2972 \nL 1050 2972 \nL 1050 838 \nQ 1050 594 1129 495 \nQ 1209 397 1406 397 \nQ 1488 397 1564 409 \nQ 1641 422 1731 441 \nL 1731 25 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-61\"/>\n       <use xlink:href=\"#LiberationSans-74\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-74\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 222.164839 307.164634 \nL 222.164839 41.052634 \n\" clip-path=\"url(#pa76db48965)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_10\"/>\n     <g id=\"text_5\">\n      <!-- få -->\n      <g style=\"fill: #262626\" transform=\"translate(220.633002 35.963092) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-66\" d=\"M 1128 2972 \nL 1128 0 \nL 566 0 \nL 566 2972 \nL 91 2972 \nL 91 3381 \nL 566 3381 \nL 566 3763 \nQ 566 3947 600 4105 \nQ 634 4263 726 4380 \nQ 819 4497 978 4564 \nQ 1138 4631 1391 4631 \nQ 1491 4631 1598 4622 \nQ 1706 4613 1788 4594 \nL 1788 4166 \nQ 1734 4175 1664 4183 \nQ 1594 4191 1538 4191 \nQ 1413 4191 1333 4156 \nQ 1253 4122 1208 4058 \nQ 1163 3994 1145 3900 \nQ 1128 3806 1128 3684 \nL 1128 3381 \nL 1788 3381 \nL 1788 2972 \nL 1128 2972 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-66\"/>\n       <use xlink:href=\"#LiberationSans-e5\" x=\"27.783203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path d=\"M 262.978554 307.164634 \nL 262.978554 41.052634 \n\" clip-path=\"url(#pa76db48965)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_12\"/>\n     <g id=\"text_6\">\n      <!-- hjälp -->\n      <g style=\"fill: #262626\" transform=\"translate(256.548819 35.963092) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-68\" d=\"M 991 2803 \nQ 1084 2975 1193 3095 \nQ 1303 3216 1434 3294 \nQ 1566 3372 1722 3408 \nQ 1878 3444 2072 3444 \nQ 2397 3444 2605 3356 \nQ 2813 3269 2933 3111 \nQ 3053 2953 3098 2734 \nQ 3144 2516 3144 2253 \nL 3144 0 \nL 2578 0 \nL 2578 2144 \nQ 2578 2359 2551 2521 \nQ 2525 2684 2450 2792 \nQ 2375 2900 2237 2954 \nQ 2100 3009 1881 3009 \nQ 1681 3009 1520 2937 \nQ 1359 2866 1245 2734 \nQ 1131 2603 1068 2415 \nQ 1006 2228 1006 1994 \nL 1006 0 \nL 444 0 \nL 444 4638 \nL 1006 4638 \nL 1006 3431 \nQ 1006 3328 1003 3225 \nQ 1000 3122 995 3034 \nQ 991 2947 987 2886 \nQ 984 2825 981 2803 \nL 991 2803 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6a\" d=\"M 428 4100 \nL 428 4638 \nL 991 4638 \nL 991 4100 \nL 428 4100 \nz\nM 991 -419 \nQ 991 -613 958 -778 \nQ 925 -944 840 -1066 \nQ 756 -1188 611 -1258 \nQ 466 -1328 241 -1328 \nQ 134 -1328 32 -1322 \nQ -69 -1316 -156 -1300 \nL -156 -866 \nQ -116 -872 -59 -878 \nQ -3 -884 38 -884 \nQ 156 -884 232 -853 \nQ 309 -822 353 -755 \nQ 397 -688 412 -583 \nQ 428 -478 428 -334 \nL 428 3381 \nL 991 3381 \nL 991 -419 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-e4\" d=\"M 1294 -63 \nQ 784 -63 528 206 \nQ 272 475 272 944 \nQ 272 1278 398 1492 \nQ 525 1706 729 1828 \nQ 934 1950 1196 1997 \nQ 1459 2044 1731 2050 \nL 2491 2063 \nL 2491 2247 \nQ 2491 2456 2447 2603 \nQ 2403 2750 2312 2840 \nQ 2222 2931 2086 2973 \nQ 1950 3016 1766 3016 \nQ 1603 3016 1472 2992 \nQ 1341 2969 1244 2908 \nQ 1147 2847 1087 2742 \nQ 1028 2638 1009 2478 \nL 422 2531 \nQ 453 2731 540 2898 \nQ 628 3066 789 3187 \nQ 950 3309 1192 3376 \nQ 1434 3444 1778 3444 \nQ 2416 3444 2737 3151 \nQ 3059 2859 3059 2306 \nL 3059 850 \nQ 3059 600 3125 473 \nQ 3191 347 3375 347 \nQ 3422 347 3469 353 \nQ 3516 359 3559 369 \nL 3559 19 \nQ 3453 -6 3348 -18 \nQ 3244 -31 3125 -31 \nQ 2966 -31 2852 11 \nQ 2738 53 2666 139 \nQ 2594 225 2556 351 \nQ 2519 478 2509 647 \nL 2491 647 \nQ 2400 484 2292 353 \nQ 2184 222 2040 130 \nQ 1897 38 1714 -12 \nQ 1531 -63 1294 -63 \nz\nM 1422 359 \nQ 1691 359 1892 457 \nQ 2094 556 2226 709 \nQ 2359 863 2425 1044 \nQ 2491 1225 2491 1391 \nL 2491 1669 \nL 1875 1656 \nQ 1669 1653 1483 1626 \nQ 1297 1600 1156 1522 \nQ 1016 1444 933 1303 \nQ 850 1163 850 934 \nQ 850 659 998 509 \nQ 1147 359 1422 359 \nz\nM 2108 3809 \nL 2108 4384 \nL 2617 4384 \nL 2617 3809 \nL 2108 3809 \nz\nM 877 3809 \nL 877 4384 \nL 1392 4384 \nL 1392 3809 \nL 877 3809 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6c\" d=\"M 431 0 \nL 431 4638 \nL 994 4638 \nL 994 0 \nL 431 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-70\" d=\"M 3291 1706 \nQ 3291 1322 3228 997 \nQ 3166 672 3020 437 \nQ 2875 203 2636 70 \nQ 2397 -63 2047 -63 \nQ 1684 -63 1412 75 \nQ 1141 213 997 525 \nL 981 525 \nQ 984 519 986 469 \nQ 988 419 989 344 \nQ 991 269 992 176 \nQ 994 84 994 -6 \nL 994 -1328 \nL 431 -1328 \nL 431 2691 \nQ 431 2813 429 2925 \nQ 428 3038 425 3130 \nQ 422 3222 419 3287 \nQ 416 3353 413 3381 \nL 956 3381 \nQ 959 3372 964 3315 \nQ 969 3259 973 3179 \nQ 978 3100 983 3009 \nQ 988 2919 988 2838 \nL 1000 2838 \nQ 1078 3000 1178 3114 \nQ 1278 3228 1406 3301 \nQ 1534 3375 1692 3408 \nQ 1850 3441 2047 3441 \nQ 2397 3441 2636 3316 \nQ 2875 3191 3020 2964 \nQ 3166 2738 3228 2417 \nQ 3291 2097 3291 1706 \nz\nM 2700 1694 \nQ 2700 2006 2662 2250 \nQ 2625 2494 2533 2662 \nQ 2441 2831 2287 2918 \nQ 2134 3006 1903 3006 \nQ 1716 3006 1550 2953 \nQ 1384 2900 1261 2750 \nQ 1138 2600 1066 2336 \nQ 994 2072 994 1650 \nQ 994 1291 1053 1042 \nQ 1113 794 1227 641 \nQ 1341 488 1509 420 \nQ 1678 353 1897 353 \nQ 2131 353 2286 443 \nQ 2441 534 2533 706 \nQ 2625 878 2662 1126 \nQ 2700 1375 2700 1694 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-68\"/>\n       <use xlink:href=\"#LiberationSans-6a\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-e4\" x=\"77.832031\"/>\n       <use xlink:href=\"#LiberationSans-6c\" x=\"133.447266\"/>\n       <use xlink:href=\"#LiberationSans-70\" x=\"155.664062\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path d=\"M 303.792268 307.164634 \nL 303.792268 41.052634 \n\" clip-path=\"url(#pa76db48965)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_14\"/>\n     <g id=\"text_7\">\n      <!-- . -->\n      <g style=\"fill: #262626\" transform=\"translate(304.390393 35.963092) rotate(-40) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-2e\" d=\"M 584 0 \nL 584 684 \nL 1194 684 \nL 1194 0 \nL 584 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-2e\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_15\">\n      <path d=\"M 38.503125 60.060634 \nL 324.199125 60.060634 \n\" clip-path=\"url(#pa76db48965)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_16\"/>\n     <g id=\"text_8\">\n      <!-- time -->\n      <g style=\"fill: #262626\" transform=\"translate(16.1125 63.684071) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-69\" d=\"M 428 4100 \nL 428 4638 \nL 991 4638 \nL 991 4100 \nL 428 4100 \nz\nM 428 0 \nL 428 3381 \nL 991 3381 \nL 991 0 \nL 428 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-74\"/>\n       <use xlink:href=\"#LiberationSans-69\" x=\"27.783203\"/>\n       <use xlink:href=\"#LiberationSans-6d\" x=\"50\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"133.300781\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_17\">\n      <path d=\"M 38.503125 98.076634 \nL 324.199125 98.076634 \n\" clip-path=\"url(#pa76db48965)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_18\"/>\n     <g id=\"text_9\">\n      <!-- is -->\n      <g style=\"fill: #262626\" transform=\"translate(27.78125 101.700071) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-73\" d=\"M 2969 934 \nQ 2969 697 2876 511 \nQ 2784 325 2609 198 \nQ 2434 72 2179 4 \nQ 1925 -63 1597 -63 \nQ 1303 -63 1067 -17 \nQ 831 28 653 128 \nQ 475 228 354 392 \nQ 234 556 178 794 \nL 675 891 \nQ 747 619 972 492 \nQ 1197 366 1597 366 \nQ 1778 366 1929 391 \nQ 2081 416 2190 477 \nQ 2300 538 2361 639 \nQ 2422 741 2422 891 \nQ 2422 1044 2350 1142 \nQ 2278 1241 2150 1306 \nQ 2022 1372 1839 1420 \nQ 1656 1469 1438 1528 \nQ 1234 1581 1034 1647 \nQ 834 1713 673 1820 \nQ 513 1928 413 2087 \nQ 313 2247 313 2488 \nQ 313 2950 642 3192 \nQ 972 3434 1603 3434 \nQ 2163 3434 2492 3237 \nQ 2822 3041 2909 2606 \nL 2403 2544 \nQ 2375 2675 2300 2764 \nQ 2225 2853 2119 2908 \nQ 2013 2963 1880 2986 \nQ 1747 3009 1603 3009 \nQ 1222 3009 1040 2893 \nQ 859 2778 859 2544 \nQ 859 2406 926 2317 \nQ 994 2228 1114 2167 \nQ 1234 2106 1403 2061 \nQ 1572 2016 1775 1966 \nQ 1909 1931 2050 1892 \nQ 2191 1853 2323 1798 \nQ 2456 1744 2573 1670 \nQ 2691 1597 2778 1494 \nQ 2866 1391 2917 1253 \nQ 2969 1116 2969 934 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-69\"/>\n       <use xlink:href=\"#LiberationSans-73\" x=\"22.216797\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <path d=\"M 38.503125 136.092634 \nL 324.199125 136.092634 \n\" clip-path=\"url(#pa76db48965)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_20\"/>\n     <g id=\"text_10\">\n      <!-- the -->\n      <g style=\"fill: #262626\" transform=\"translate(21.103125 139.716071) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-74\"/>\n       <use xlink:href=\"#LiberationSans-68\" x=\"27.783203\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_21\">\n      <path d=\"M 38.503125 174.108634 \nL 324.199125 174.108634 \n\" clip-path=\"url(#pa76db48965)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_22\"/>\n     <g id=\"text_11\">\n      <!-- &lt;unk&gt; -->\n      <g style=\"fill: #262626\" transform=\"translate(7.2 177.732071) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-3c\" d=\"M 316 1784 \nL 316 2425 \nL 3425 3731 \nL 3425 3250 \nL 744 2106 \nL 3425 959 \nL 3425 481 \nL 316 1784 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-75\" d=\"M 981 3381 \nL 981 1238 \nQ 981 991 1017 825 \nQ 1053 659 1134 557 \nQ 1216 456 1348 414 \nQ 1481 372 1678 372 \nQ 1878 372 2039 442 \nQ 2200 513 2314 645 \nQ 2428 778 2490 973 \nQ 2553 1169 2553 1422 \nL 2553 3381 \nL 3116 3381 \nL 3116 722 \nQ 3116 616 3117 498 \nQ 3119 381 3122 276 \nQ 3125 172 3128 97 \nQ 3131 22 3134 0 \nL 2603 0 \nQ 2600 16 2597 84 \nQ 2594 153 2589 242 \nQ 2584 331 2581 423 \nQ 2578 516 2578 578 \nL 2569 578 \nQ 2488 431 2391 312 \nQ 2294 194 2166 111 \nQ 2038 28 1872 -17 \nQ 1706 -63 1488 -63 \nQ 1206 -63 1003 6 \nQ 800 75 669 219 \nQ 538 363 477 588 \nQ 416 813 416 1128 \nL 416 3381 \nL 981 3381 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-6e\" d=\"M 2578 0 \nL 2578 2144 \nQ 2578 2391 2542 2556 \nQ 2506 2722 2425 2823 \nQ 2344 2925 2211 2967 \nQ 2078 3009 1881 3009 \nQ 1681 3009 1520 2939 \nQ 1359 2869 1245 2736 \nQ 1131 2603 1068 2408 \nQ 1006 2213 1006 1959 \nL 1006 0 \nL 444 0 \nL 444 2659 \nQ 444 2766 442 2883 \nQ 441 3000 437 3104 \nQ 434 3209 431 3284 \nQ 428 3359 425 3381 \nL 956 3381 \nQ 959 3366 962 3297 \nQ 966 3228 970 3139 \nQ 975 3050 978 2958 \nQ 981 2866 981 2803 \nL 991 2803 \nQ 1072 2950 1169 3069 \nQ 1266 3188 1394 3270 \nQ 1522 3353 1687 3398 \nQ 1853 3444 2072 3444 \nQ 2353 3444 2556 3375 \nQ 2759 3306 2890 3162 \nQ 3022 3019 3083 2792 \nQ 3144 2566 3144 2253 \nL 3144 0 \nL 2578 0 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"LiberationSans-3e\" d=\"M 316 481 \nL 316 959 \nL 2997 2106 \nL 316 3250 \nL 316 3731 \nL 3425 2425 \nL 3425 1784 \nL 316 481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-3c\"/>\n       <use xlink:href=\"#LiberationSans-75\" x=\"58.398438\"/>\n       <use xlink:href=\"#LiberationSans-6e\" x=\"114.013672\"/>\n       <use xlink:href=\"#LiberationSans-6b\" x=\"169.628906\"/>\n       <use xlink:href=\"#LiberationSans-3e\" x=\"219.628906\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <path d=\"M 38.503125 212.124634 \nL 324.199125 212.124634 \n\" clip-path=\"url(#pa76db48965)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_24\"/>\n     <g id=\"text_12\">\n      <!-- to -->\n      <g style=\"fill: #262626\" transform=\"translate(26.664062 215.748071) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-74\"/>\n       <use xlink:href=\"#LiberationSans-6f\" x=\"27.783203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_25\">\n      <path d=\"M 38.503125 250.140634 \nL 324.199125 250.140634 \n\" clip-path=\"url(#pa76db48965)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_26\"/>\n     <g id=\"text_13\">\n      <!-- help -->\n      <g style=\"fill: #262626\" transform=\"translate(16.098437 253.764071) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-68\"/>\n       <use xlink:href=\"#LiberationSans-65\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-6c\" x=\"111.230469\"/>\n       <use xlink:href=\"#LiberationSans-70\" x=\"133.447266\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_27\">\n      <path d=\"M 38.503125 288.156634 \nL 324.199125 288.156634 \n\" clip-path=\"url(#pa76db48965)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"line2d_28\"/>\n     <g id=\"text_14\">\n      <!-- . -->\n      <g style=\"fill: #262626\" transform=\"translate(32.225 291.780071) scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-2e\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"PolyCollection_1\">\n    <path d=\"M 38.503125 41.052634 \nL 38.503125 79.068634 \nL 79.316839 79.068634 \nL 79.316839 41.052634 \nL 38.503125 41.052634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #3585bf\"/>\n    <path d=\"M 79.316839 41.052634 \nL 79.316839 79.068634 \nL 120.130554 79.068634 \nL 120.130554 41.052634 \nL 79.316839 41.052634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #083674\"/>\n    <path d=\"M 120.130554 41.052634 \nL 120.130554 79.068634 \nL 160.944268 79.068634 \nL 160.944268 41.052634 \nL 120.130554 41.052634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #2b7bba\"/>\n    <path d=\"M 160.944268 41.052634 \nL 160.944268 79.068634 \nL 201.757982 79.068634 \nL 201.757982 41.052634 \nL 160.944268 41.052634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #084b93\"/>\n    <path d=\"M 201.757982 41.052634 \nL 201.757982 79.068634 \nL 242.571696 79.068634 \nL 242.571696 41.052634 \nL 201.757982 41.052634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #084387\"/>\n    <path d=\"M 242.571696 41.052634 \nL 242.571696 79.068634 \nL 283.385411 79.068634 \nL 283.385411 41.052634 \nL 242.571696 41.052634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #084082\"/>\n    <path d=\"M 283.385411 41.052634 \nL 283.385411 79.068634 \nL 324.199125 79.068634 \nL 324.199125 41.052634 \nL 283.385411 41.052634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #084f99\"/>\n    <path d=\"M 38.503125 79.068634 \nL 38.503125 117.084634 \nL 79.316839 117.084634 \nL 79.316839 79.068634 \nL 38.503125 79.068634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #3080bd\"/>\n    <path d=\"M 79.316839 79.068634 \nL 79.316839 117.084634 \nL 120.130554 117.084634 \nL 120.130554 79.068634 \nL 79.316839 79.068634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #083471\"/>\n    <path d=\"M 120.130554 79.068634 \nL 120.130554 117.084634 \nL 160.944268 117.084634 \nL 160.944268 79.068634 \nL 120.130554 79.068634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #77b5d9\"/>\n    <path d=\"M 160.944268 79.068634 \nL 160.944268 117.084634 \nL 201.757982 117.084634 \nL 201.757982 79.068634 \nL 160.944268 79.068634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #08458a\"/>\n    <path d=\"M 201.757982 79.068634 \nL 201.757982 117.084634 \nL 242.571696 117.084634 \nL 242.571696 79.068634 \nL 201.757982 79.068634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #083370\"/>\n    <path d=\"M 242.571696 79.068634 \nL 242.571696 117.084634 \nL 283.385411 117.084634 \nL 283.385411 79.068634 \nL 242.571696 79.068634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #083370\"/>\n    <path d=\"M 283.385411 79.068634 \nL 283.385411 117.084634 \nL 324.199125 117.084634 \nL 324.199125 79.068634 \nL 283.385411 79.068634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #083979\"/>\n    <path d=\"M 38.503125 117.084634 \nL 38.503125 155.100634 \nL 79.316839 155.100634 \nL 79.316839 117.084634 \nL 38.503125 117.084634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #084990\"/>\n    <path d=\"M 79.316839 117.084634 \nL 79.316839 155.100634 \nL 120.130554 155.100634 \nL 120.130554 117.084634 \nL 79.316839 117.084634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #08326e\"/>\n    <path d=\"M 120.130554 117.084634 \nL 120.130554 155.100634 \nL 160.944268 155.100634 \nL 160.944268 117.084634 \nL 120.130554 117.084634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #a9cfe5\"/>\n    <path d=\"M 160.944268 117.084634 \nL 160.944268 155.100634 \nL 201.757982 155.100634 \nL 201.757982 117.084634 \nL 160.944268 117.084634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #0a539e\"/>\n    <path d=\"M 201.757982 117.084634 \nL 201.757982 155.100634 \nL 242.571696 155.100634 \nL 242.571696 117.084634 \nL 201.757982 117.084634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #083776\"/>\n    <path d=\"M 242.571696 117.084634 \nL 242.571696 155.100634 \nL 283.385411 155.100634 \nL 283.385411 117.084634 \nL 242.571696 117.084634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #083c7d\"/>\n    <path d=\"M 283.385411 117.084634 \nL 283.385411 155.100634 \nL 324.199125 155.100634 \nL 324.199125 117.084634 \nL 283.385411 117.084634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #083674\"/>\n    <path d=\"M 38.503125 155.100634 \nL 38.503125 193.116634 \nL 79.316839 193.116634 \nL 79.316839 155.100634 \nL 38.503125 155.100634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #083471\"/>\n    <path d=\"M 79.316839 155.100634 \nL 79.316839 193.116634 \nL 120.130554 193.116634 \nL 120.130554 155.100634 \nL 79.316839 155.100634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #08316d\"/>\n    <path d=\"M 120.130554 155.100634 \nL 120.130554 193.116634 \nL 160.944268 193.116634 \nL 160.944268 155.100634 \nL 120.130554 155.100634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #1865ac\"/>\n    <path d=\"M 160.944268 155.100634 \nL 160.944268 193.116634 \nL 201.757982 193.116634 \nL 201.757982 155.100634 \nL 160.944268 155.100634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #084b93\"/>\n    <path d=\"M 201.757982 155.100634 \nL 201.757982 193.116634 \nL 242.571696 193.116634 \nL 242.571696 155.100634 \nL 201.757982 155.100634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #2272b6\"/>\n    <path d=\"M 242.571696 155.100634 \nL 242.571696 193.116634 \nL 283.385411 193.116634 \nL 283.385411 155.100634 \nL 242.571696 155.100634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #2676b8\"/>\n    <path d=\"M 283.385411 155.100634 \nL 283.385411 193.116634 \nL 324.199125 193.116634 \nL 324.199125 155.100634 \nL 283.385411 155.100634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #0b559f\"/>\n    <path d=\"M 38.503125 193.116634 \nL 38.503125 231.132634 \nL 79.316839 231.132634 \nL 79.316839 193.116634 \nL 38.503125 193.116634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #083370\"/>\n    <path d=\"M 79.316839 193.116634 \nL 79.316839 231.132634 \nL 120.130554 231.132634 \nL 120.130554 193.116634 \nL 79.316839 193.116634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #08326e\"/>\n    <path d=\"M 120.130554 193.116634 \nL 120.130554 231.132634 \nL 160.944268 231.132634 \nL 160.944268 193.116634 \nL 120.130554 193.116634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #3787c0\"/>\n    <path d=\"M 160.944268 193.116634 \nL 160.944268 231.132634 \nL 201.757982 231.132634 \nL 201.757982 193.116634 \nL 160.944268 193.116634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #1e6db2\"/>\n    <path d=\"M 201.757982 193.116634 \nL 201.757982 231.132634 \nL 242.571696 231.132634 \nL 242.571696 193.116634 \nL 201.757982 193.116634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #084d96\"/>\n    <path d=\"M 242.571696 193.116634 \nL 242.571696 231.132634 \nL 283.385411 231.132634 \nL 283.385411 193.116634 \nL 242.571696 193.116634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #084b93\"/>\n    <path d=\"M 283.385411 193.116634 \nL 283.385411 231.132634 \nL 324.199125 231.132634 \nL 324.199125 193.116634 \nL 283.385411 193.116634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #1561a9\"/>\n    <path d=\"M 38.503125 231.132634 \nL 38.503125 269.148634 \nL 79.316839 269.148634 \nL 79.316839 231.132634 \nL 38.503125 231.132634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #08316d\"/>\n    <path d=\"M 79.316839 231.132634 \nL 79.316839 269.148634 \nL 120.130554 269.148634 \nL 120.130554 231.132634 \nL 79.316839 231.132634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #08306b\"/>\n    <path d=\"M 120.130554 231.132634 \nL 120.130554 269.148634 \nL 160.944268 269.148634 \nL 160.944268 231.132634 \nL 120.130554 231.132634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #083c7d\"/>\n    <path d=\"M 160.944268 231.132634 \nL 160.944268 269.148634 \nL 201.757982 269.148634 \nL 201.757982 231.132634 \nL 160.944268 231.132634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #084b93\"/>\n    <path d=\"M 201.757982 231.132634 \nL 201.757982 269.148634 \nL 242.571696 269.148634 \nL 242.571696 231.132634 \nL 201.757982 231.132634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #08519c\"/>\n    <path d=\"M 242.571696 231.132634 \nL 242.571696 269.148634 \nL 283.385411 269.148634 \nL 283.385411 231.132634 \nL 242.571696 231.132634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #58a1cf\"/>\n    <path d=\"M 283.385411 231.132634 \nL 283.385411 269.148634 \nL 324.199125 269.148634 \nL 324.199125 231.132634 \nL 283.385411 231.132634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #2676b8\"/>\n    <path d=\"M 38.503125 269.148634 \nL 38.503125 307.164634 \nL 79.316839 307.164634 \nL 79.316839 269.148634 \nL 38.503125 269.148634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #08316d\"/>\n    <path d=\"M 79.316839 269.148634 \nL 79.316839 307.164634 \nL 120.130554 307.164634 \nL 120.130554 269.148634 \nL 79.316839 269.148634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #08306b\"/>\n    <path d=\"M 120.130554 269.148634 \nL 120.130554 307.164634 \nL 160.944268 307.164634 \nL 160.944268 269.148634 \nL 120.130554 269.148634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #084082\"/>\n    <path d=\"M 160.944268 269.148634 \nL 160.944268 307.164634 \nL 201.757982 307.164634 \nL 201.757982 269.148634 \nL 160.944268 269.148634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #083877\"/>\n    <path d=\"M 201.757982 269.148634 \nL 201.757982 307.164634 \nL 242.571696 307.164634 \nL 242.571696 269.148634 \nL 201.757982 269.148634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #084a91\"/>\n    <path d=\"M 242.571696 269.148634 \nL 242.571696 307.164634 \nL 283.385411 307.164634 \nL 283.385411 269.148634 \nL 242.571696 269.148634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #3484bf\"/>\n    <path d=\"M 283.385411 269.148634 \nL 283.385411 307.164634 \nL 324.199125 307.164634 \nL 324.199125 269.148634 \nL 283.385411 269.148634 \nz\n\" clip-path=\"url(#pa76db48965)\" style=\"fill: #64a9d3\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 38.503125 307.164634 \nL 38.503125 41.052634 \n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 324.199125 307.164634 \nL 324.199125 41.052634 \n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 38.503125 307.164634 \nL 324.199125 307.164634 \n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 38.503125 41.052634 \nL 324.199125 41.052634 \n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 342.055125 307.164634 \nL 355.360725 307.164634 \nL 355.360725 41.052634 \nL 342.055125 41.052634 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_8\">\n     <g id=\"line2d_29\"/>\n     <g id=\"text_15\">\n      <!-- 0.0 -->\n      <g style=\"fill: #262626\" transform=\"translate(358.860725 310.788071) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-30\" d=\"M 3309 2203 \nQ 3309 1569 3189 1136 \nQ 3069 703 2861 436 \nQ 2653 169 2372 53 \nQ 2091 -63 1772 -63 \nQ 1450 -63 1172 53 \nQ 894 169 689 434 \nQ 484 700 367 1133 \nQ 250 1566 250 2203 \nQ 250 2869 367 3305 \nQ 484 3741 690 4000 \nQ 897 4259 1178 4364 \nQ 1459 4469 1791 4469 \nQ 2106 4469 2382 4364 \nQ 2659 4259 2865 4000 \nQ 3072 3741 3190 3305 \nQ 3309 2869 3309 2203 \nz\nM 2738 2203 \nQ 2738 2728 2675 3076 \nQ 2613 3425 2491 3633 \nQ 2369 3841 2192 3927 \nQ 2016 4013 1791 4013 \nQ 1553 4013 1372 3925 \nQ 1191 3838 1067 3630 \nQ 944 3422 881 3073 \nQ 819 2725 819 2203 \nQ 819 1697 883 1350 \nQ 947 1003 1070 792 \nQ 1194 581 1372 489 \nQ 1550 397 1778 397 \nQ 2000 397 2178 489 \nQ 2356 581 2479 792 \nQ 2603 1003 2670 1350 \nQ 2738 1697 2738 2203 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-30\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_30\"/>\n     <g id=\"text_16\">\n      <!-- 0.2 -->\n      <g style=\"fill: #262626\" transform=\"translate(358.860725 257.565671) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-32\" d=\"M 322 0 \nL 322 397 \nQ 481 763 711 1042 \nQ 941 1322 1194 1548 \nQ 1447 1775 1695 1969 \nQ 1944 2163 2144 2356 \nQ 2344 2550 2467 2762 \nQ 2591 2975 2591 3244 \nQ 2591 3431 2534 3573 \nQ 2478 3716 2372 3812 \nQ 2266 3909 2117 3957 \nQ 1969 4006 1788 4006 \nQ 1619 4006 1470 3959 \nQ 1322 3913 1206 3819 \nQ 1091 3725 1017 3586 \nQ 944 3447 922 3263 \nL 347 3316 \nQ 375 3553 478 3762 \nQ 581 3972 762 4130 \nQ 944 4288 1198 4378 \nQ 1453 4469 1788 4469 \nQ 2116 4469 2372 4391 \nQ 2628 4313 2804 4159 \nQ 2981 4006 3075 3781 \nQ 3169 3556 3169 3263 \nQ 3169 3041 3089 2841 \nQ 3009 2641 2876 2459 \nQ 2744 2278 2569 2109 \nQ 2394 1941 2203 1780 \nQ 2013 1619 1819 1461 \nQ 1625 1303 1454 1143 \nQ 1284 984 1150 820 \nQ 1016 656 941 478 \nL 3238 478 \nL 3238 0 \nL 322 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-32\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_31\"/>\n     <g id=\"text_17\">\n      <!-- 0.4 -->\n      <g style=\"fill: #262626\" transform=\"translate(358.860725 204.343271) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-34\" d=\"M 2753 997 \nL 2753 0 \nL 2222 0 \nL 2222 997 \nL 147 997 \nL 147 1434 \nL 2163 4403 \nL 2753 4403 \nL 2753 1441 \nL 3372 1441 \nL 3372 997 \nL 2753 997 \nz\nM 2222 3769 \nQ 2216 3753 2191 3708 \nQ 2166 3663 2134 3606 \nQ 2103 3550 2070 3492 \nQ 2038 3434 2013 3397 \nL 884 1734 \nQ 869 1709 839 1668 \nQ 809 1628 778 1586 \nQ 747 1544 715 1503 \nQ 684 1463 666 1441 \nL 2222 1441 \nL 2222 3769 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-34\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_32\"/>\n     <g id=\"text_18\">\n      <!-- 0.6 -->\n      <g style=\"fill: #262626\" transform=\"translate(358.860725 151.120871) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-36\" d=\"M 3278 1441 \nQ 3278 1109 3186 832 \nQ 3094 556 2914 357 \nQ 2734 159 2468 48 \nQ 2203 -63 1856 -63 \nQ 1472 -63 1184 84 \nQ 897 231 706 507 \nQ 516 784 420 1186 \nQ 325 1588 325 2100 \nQ 325 2688 433 3131 \nQ 541 3575 744 3872 \nQ 947 4169 1239 4319 \nQ 1531 4469 1900 4469 \nQ 2125 4469 2322 4422 \nQ 2519 4375 2680 4270 \nQ 2841 4166 2962 3994 \nQ 3084 3822 3156 3572 \nL 2619 3475 \nQ 2531 3759 2339 3886 \nQ 2147 4013 1894 4013 \nQ 1663 4013 1475 3903 \nQ 1288 3794 1156 3576 \nQ 1025 3359 954 3031 \nQ 884 2703 884 2266 \nQ 1038 2550 1316 2698 \nQ 1594 2847 1953 2847 \nQ 2253 2847 2497 2750 \nQ 2741 2653 2914 2470 \nQ 3088 2288 3183 2027 \nQ 3278 1766 3278 1441 \nz\nM 2706 1416 \nQ 2706 1644 2650 1828 \nQ 2594 2013 2481 2142 \nQ 2369 2272 2203 2342 \nQ 2038 2413 1819 2413 \nQ 1666 2413 1509 2367 \nQ 1353 2322 1226 2220 \nQ 1100 2119 1020 1953 \nQ 941 1788 941 1550 \nQ 941 1306 1003 1095 \nQ 1066 884 1183 728 \nQ 1300 572 1465 481 \nQ 1631 391 1838 391 \nQ 2041 391 2202 461 \nQ 2363 531 2475 664 \nQ 2588 797 2647 987 \nQ 2706 1178 2706 1416 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-36\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_33\"/>\n     <g id=\"text_19\">\n      <!-- 0.8 -->\n      <g style=\"fill: #262626\" transform=\"translate(358.860725 97.898471) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-38\" d=\"M 3281 1228 \nQ 3281 947 3192 711 \nQ 3103 475 2920 303 \nQ 2738 131 2453 34 \nQ 2169 -63 1781 -63 \nQ 1394 -63 1111 34 \nQ 828 131 642 301 \nQ 456 472 367 708 \nQ 278 944 278 1222 \nQ 278 1463 351 1650 \nQ 425 1838 548 1973 \nQ 672 2109 830 2192 \nQ 988 2275 1156 2303 \nL 1156 2316 \nQ 972 2359 826 2456 \nQ 681 2553 582 2689 \nQ 484 2825 432 2990 \nQ 381 3156 381 3341 \nQ 381 3572 470 3776 \nQ 559 3981 734 4136 \nQ 909 4291 1168 4380 \nQ 1428 4469 1769 4469 \nQ 2128 4469 2392 4378 \nQ 2656 4288 2829 4133 \nQ 3003 3978 3087 3772 \nQ 3172 3566 3172 3334 \nQ 3172 3153 3120 2987 \nQ 3069 2822 2970 2686 \nQ 2872 2550 2726 2454 \nQ 2581 2359 2391 2322 \nL 2391 2309 \nQ 2581 2278 2743 2195 \nQ 2906 2113 3025 1977 \nQ 3144 1841 3212 1653 \nQ 3281 1466 3281 1228 \nz\nM 2588 3303 \nQ 2588 3469 2545 3606 \nQ 2503 3744 2406 3842 \nQ 2309 3941 2153 3995 \nQ 1997 4050 1769 4050 \nQ 1547 4050 1394 3995 \nQ 1241 3941 1142 3842 \nQ 1044 3744 1000 3606 \nQ 956 3469 956 3303 \nQ 956 3172 990 3034 \nQ 1025 2897 1115 2784 \nQ 1206 2672 1365 2600 \nQ 1525 2528 1775 2528 \nQ 2041 2528 2202 2600 \nQ 2363 2672 2448 2784 \nQ 2534 2897 2561 3034 \nQ 2588 3172 2588 3303 \nz\nM 2697 1281 \nQ 2697 1441 2653 1589 \nQ 2609 1738 2503 1852 \nQ 2397 1966 2217 2036 \nQ 2038 2106 1769 2106 \nQ 1522 2106 1348 2036 \nQ 1175 1966 1067 1850 \nQ 959 1734 909 1582 \nQ 859 1431 859 1269 \nQ 859 1066 909 898 \nQ 959 731 1068 611 \nQ 1178 491 1356 425 \nQ 1534 359 1788 359 \nQ 2044 359 2219 425 \nQ 2394 491 2500 611 \nQ 2606 731 2651 901 \nQ 2697 1072 2697 1281 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-30\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-38\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_34\"/>\n     <g id=\"text_20\">\n      <!-- 1.0 -->\n      <g style=\"fill: #262626\" transform=\"translate(358.860725 44.676071) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"LiberationSans-31\" d=\"M 488 0 \nL 488 478 \nL 1609 478 \nL 1609 3866 \nL 616 3156 \nL 616 3688 \nL 1656 4403 \nL 2175 4403 \nL 2175 478 \nL 3247 478 \nL 3247 0 \nL 488 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#LiberationSans-31\"/>\n       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#LiberationSans-30\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAABMAAAFyCAYAAAAXj+GHAAABx0lEQVR4nO2cwW0EMQwD7cU+Uk76Sv/vSw0SBrgBSBZALEWJsn1I7s/v3+dAeM99KK7DMZ1z3nMvSVaZQ4TIRL+MrlllzslCZGoH/emgD1GZK7LKHCJEpjkcrTWrzDlSZF5yNlGyw3Gl1Kwy50D7TCxT66a4Zm2N75LVzTm8bopr1tb4LlndnMPrprhmbY0NGcZVmQt4Zb4P+HsAXTOUrDKHCJmAEJnq1kDJKnOIypzDKzMnHNsaQ1TmHPBpu4M+JgMvKGaZYjfBJ5e6uSAjU0Mcjt0BY9CDTpJpZZIR5E0NcTim7ACMip8AadOKUyNlB2S4SbKZZbJNC5KF1My70dEdQH6aOM9CYhtOjdZsiD7fzNHttCALCUdtzegzrfRWJ3bT27Ta2TS3Bknm3QHoO614AlAybdOyT6sYVc4EeGXWzQWZdtC15zP0img+BWXI9E5AyFMEKjNk0Nk7ulYm7CYH8Y8LXpns3yJaU0N82BP/h5m6OSarmwsyjislHLvR56ibGzKOq+G4QFfdHHVzQdZwXJBxXF11C6S4ico8Dcc5mVgm6KZ5o5NkYplSN1PCMWPVoV/mlRnipvceIA5H7TjVzQ2ZVeY/J9EI0K+fCH8AAAAASUVORK5CYII=\" id=\"image2a5d828785\" transform=\"scale(1 -1) translate(0 -266.4)\" x=\"342\" y=\"-40.32\" width=\"13.68\" height=\"266.4\"/>\n   <g id=\"LineCollection_1\"/>\n   <g id=\"patch_8\">\n    <path d=\"M 342.055125 307.164634 \nL 348.707925 307.164634 \nL 355.360725 307.164634 \nL 355.360725 41.052634 \nL 348.707925 41.052634 \nL 342.055125 41.052634 \nL 342.055125 307.164634 \nz\n\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pa76db48965\">\n   <rect x=\"38.503125\" y=\"41.052634\" width=\"285.696\" height=\"266.112\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentense we choose is \"tom är skyldig mig en massa pengar .\"\n",
        "\n",
        "The pattern we found on the heatmap is white cells on the diagonal axis since this sentence's word order is almost identical to the English one.\n",
        "\n",
        "Attention mechanisms typically improve model performance by allowing the network to focus on relevant parts of the input selectively. This can lead to better understanding and representation of the data, and resulting in more accurate predictions/translation.\n",
        "\n",
        "However, we face mistranslation issues if the sentence is from the outside world. We choose a sentence from a reading book, and even if we change the word count from 5000 to 10000, <unk> disappears, but it can not translate properly(compare the translated sentence with the output of Google Translate). We guess that it is because of the corpus size used for training."
      ],
      "metadata": {
        "id": "vyINjhVEPSrB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkS0LiJjTICz"
      },
      "source": [
        "**🥳 Congratulations on finishing this lab! 🥳**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "interpreter": {
      "hash": "12f500e95db8c7000aae6810a3b3f88d1298056c5758c6b1fc4b18ff2c238050"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}